{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning 2nd Project\n",
    "### Authors: Guilherme Cepeda - 62931, Pedro Serrano - 54853\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second project divided in 2 parts we were given 2 files, `worms_trainset.csv` and `worms_testset.csv`, and had to answer a couple of questions:\n",
    "* Can we classify the type of worm using the information provided by the eigenworm\n",
    "series?\n",
    "* For a specific worm, how can we model its motion, i.e., the eigenworm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
    "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed beforehand that the files given had no labels meaning that we would lose the first row of both datasets. So we decided to add a new row explicitly showing the target variable `class` and the 900 time instances with `t1 to t900`.\n",
    "\n",
    "Here we load the 2 files given into 2 **Dataframes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 181 entries, 0 to 180\n",
      "Columns: 901 entries, class to t900\n",
      "dtypes: float64(901)\n",
      "memory usage: 1.2 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77 entries, 0 to 76\n",
      "Columns: 901 entries, class to t900\n",
      "dtypes: float64(901)\n",
      "memory usage: 542.1 KB\n",
      "None\n",
      "(181, 901)\n",
      "(77, 901)\n"
     ]
    }
   ],
   "source": [
    "#creates a dataframe from a file\n",
    "\n",
    "#the csv has no column names, so we have to add them\n",
    "list=['class']\n",
    "for i in range(1,901):\n",
    "    list.append('t' + str(i))\n",
    "\n",
    "#we created a list [class, t0 - t900] to represent the 901 columns we have\n",
    "#names= list makes this list as a header to the dataframe\n",
    "df_trainset = pd.read_csv(\"worms_trainset.csv\", names=list)\n",
    "\n",
    "df_testset = pd.read_csv(\"worms_testset.csv\", names=list)\n",
    "\n",
    "#info\n",
    "print(df_trainset.info())\n",
    "\n",
    "#info\n",
    "print(df_testset.info())\n",
    "\n",
    "print(df_trainset.shape)\n",
    "print(df_testset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>...</th>\n",
       "      <th>t891</th>\n",
       "      <th>t892</th>\n",
       "      <th>t893</th>\n",
       "      <th>t894</th>\n",
       "      <th>t895</th>\n",
       "      <th>t896</th>\n",
       "      <th>t897</th>\n",
       "      <th>t898</th>\n",
       "      <th>t899</th>\n",
       "      <th>t900</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>181.00000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.58011</td>\n",
       "      <td>-0.054762</td>\n",
       "      <td>-0.055049</td>\n",
       "      <td>-0.053320</td>\n",
       "      <td>-0.044472</td>\n",
       "      <td>-0.046638</td>\n",
       "      <td>-0.054311</td>\n",
       "      <td>-0.060297</td>\n",
       "      <td>-0.064308</td>\n",
       "      <td>-0.059014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097132</td>\n",
       "      <td>0.101416</td>\n",
       "      <td>0.102469</td>\n",
       "      <td>0.088442</td>\n",
       "      <td>0.087919</td>\n",
       "      <td>0.075541</td>\n",
       "      <td>0.070701</td>\n",
       "      <td>0.064898</td>\n",
       "      <td>0.064639</td>\n",
       "      <td>0.055676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.49491</td>\n",
       "      <td>1.231458</td>\n",
       "      <td>1.217009</td>\n",
       "      <td>1.212576</td>\n",
       "      <td>1.211890</td>\n",
       "      <td>1.212344</td>\n",
       "      <td>1.211266</td>\n",
       "      <td>1.216288</td>\n",
       "      <td>1.214133</td>\n",
       "      <td>1.209916</td>\n",
       "      <td>...</td>\n",
       "      <td>1.280118</td>\n",
       "      <td>1.284467</td>\n",
       "      <td>1.264096</td>\n",
       "      <td>1.265234</td>\n",
       "      <td>1.264899</td>\n",
       "      <td>1.274256</td>\n",
       "      <td>1.260681</td>\n",
       "      <td>1.257045</td>\n",
       "      <td>1.278832</td>\n",
       "      <td>1.279609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-3.739104</td>\n",
       "      <td>-3.719033</td>\n",
       "      <td>-3.731076</td>\n",
       "      <td>-3.715019</td>\n",
       "      <td>-3.755160</td>\n",
       "      <td>-3.751146</td>\n",
       "      <td>-3.803328</td>\n",
       "      <td>-3.799314</td>\n",
       "      <td>-3.731076</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.485523</td>\n",
       "      <td>-3.623585</td>\n",
       "      <td>-3.027088</td>\n",
       "      <td>-3.278430</td>\n",
       "      <td>-3.003730</td>\n",
       "      <td>-3.278430</td>\n",
       "      <td>-3.002307</td>\n",
       "      <td>-2.970360</td>\n",
       "      <td>-2.957012</td>\n",
       "      <td>-3.071337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.854008</td>\n",
       "      <td>-0.831337</td>\n",
       "      <td>-0.804410</td>\n",
       "      <td>-0.804410</td>\n",
       "      <td>-0.855404</td>\n",
       "      <td>-0.822698</td>\n",
       "      <td>-0.850160</td>\n",
       "      <td>-0.883371</td>\n",
       "      <td>-0.870900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.904257</td>\n",
       "      <td>-0.882800</td>\n",
       "      <td>-0.877030</td>\n",
       "      <td>-0.874598</td>\n",
       "      <td>-0.849661</td>\n",
       "      <td>-0.846069</td>\n",
       "      <td>-0.849122</td>\n",
       "      <td>-0.934296</td>\n",
       "      <td>-0.920635</td>\n",
       "      <td>-0.931001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.034666</td>\n",
       "      <td>-0.055647</td>\n",
       "      <td>-0.033979</td>\n",
       "      <td>-0.012762</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>-0.038728</td>\n",
       "      <td>-0.014604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101768</td>\n",
       "      <td>0.072891</td>\n",
       "      <td>0.021509</td>\n",
       "      <td>0.045957</td>\n",
       "      <td>0.011206</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>-0.004682</td>\n",
       "      <td>0.035938</td>\n",
       "      <td>0.029757</td>\n",
       "      <td>0.029757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.764857</td>\n",
       "      <td>0.701450</td>\n",
       "      <td>0.795012</td>\n",
       "      <td>0.845768</td>\n",
       "      <td>0.863678</td>\n",
       "      <td>0.849089</td>\n",
       "      <td>0.857694</td>\n",
       "      <td>0.859221</td>\n",
       "      <td>0.853187</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130934</td>\n",
       "      <td>1.173048</td>\n",
       "      <td>1.112793</td>\n",
       "      <td>1.082254</td>\n",
       "      <td>1.079085</td>\n",
       "      <td>1.094289</td>\n",
       "      <td>1.049968</td>\n",
       "      <td>1.053081</td>\n",
       "      <td>1.072255</td>\n",
       "      <td>1.081842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.482405</td>\n",
       "      <td>3.295940</td>\n",
       "      <td>3.109476</td>\n",
       "      <td>2.923011</td>\n",
       "      <td>2.736547</td>\n",
       "      <td>2.603414</td>\n",
       "      <td>2.583677</td>\n",
       "      <td>2.563941</td>\n",
       "      <td>2.542011</td>\n",
       "      <td>...</td>\n",
       "      <td>3.799677</td>\n",
       "      <td>3.934791</td>\n",
       "      <td>3.754639</td>\n",
       "      <td>4.024867</td>\n",
       "      <td>3.799677</td>\n",
       "      <td>3.777158</td>\n",
       "      <td>3.777158</td>\n",
       "      <td>3.574488</td>\n",
       "      <td>3.844715</td>\n",
       "      <td>3.574488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 901 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           class          t1          t2          t3          t4          t5  \\\n",
       "count  181.00000  181.000000  181.000000  181.000000  181.000000  181.000000   \n",
       "mean     1.58011   -0.054762   -0.055049   -0.053320   -0.044472   -0.046638   \n",
       "std      0.49491    1.231458    1.217009    1.212576    1.211890    1.212344   \n",
       "min      1.00000   -3.739104   -3.719033   -3.731076   -3.715019   -3.755160   \n",
       "25%      1.00000   -0.854008   -0.831337   -0.804410   -0.804410   -0.855404   \n",
       "50%      2.00000    0.004370   -0.034666   -0.055647   -0.033979   -0.012762   \n",
       "75%      2.00000    0.764857    0.701450    0.795012    0.845768    0.863678   \n",
       "max      2.00000    3.482405    3.295940    3.109476    2.923011    2.736547   \n",
       "\n",
       "               t6          t7          t8          t9  ...        t891  \\\n",
       "count  181.000000  181.000000  181.000000  181.000000  ...  181.000000   \n",
       "mean    -0.054311   -0.060297   -0.064308   -0.059014  ...    0.097132   \n",
       "std      1.211266    1.216288    1.214133    1.209916  ...    1.280118   \n",
       "min     -3.751146   -3.803328   -3.799314   -3.731076  ...   -3.485523   \n",
       "25%     -0.822698   -0.850160   -0.883371   -0.870900  ...   -0.904257   \n",
       "50%      0.003513    0.003733   -0.038728   -0.014604  ...    0.101768   \n",
       "75%      0.849089    0.857694    0.859221    0.853187  ...    1.130934   \n",
       "max      2.603414    2.583677    2.563941    2.542011  ...    3.799677   \n",
       "\n",
       "             t892        t893        t894        t895        t896        t897  \\\n",
       "count  181.000000  181.000000  181.000000  181.000000  181.000000  181.000000   \n",
       "mean     0.101416    0.102469    0.088442    0.087919    0.075541    0.070701   \n",
       "std      1.284467    1.264096    1.265234    1.264899    1.274256    1.260681   \n",
       "min     -3.623585   -3.027088   -3.278430   -3.003730   -3.278430   -3.002307   \n",
       "25%     -0.882800   -0.877030   -0.874598   -0.849661   -0.846069   -0.849122   \n",
       "50%      0.072891    0.021509    0.045957    0.011206    0.016229   -0.004682   \n",
       "75%      1.173048    1.112793    1.082254    1.079085    1.094289    1.049968   \n",
       "max      3.934791    3.754639    4.024867    3.799677    3.777158    3.777158   \n",
       "\n",
       "             t898        t899        t900  \n",
       "count  181.000000  181.000000  181.000000  \n",
       "mean     0.064898    0.064639    0.055676  \n",
       "std      1.257045    1.278832    1.279609  \n",
       "min     -2.970360   -2.957012   -3.071337  \n",
       "25%     -0.934296   -0.920635   -0.931001  \n",
       "50%      0.035938    0.029757    0.029757  \n",
       "75%      1.053081    1.072255    1.081842  \n",
       "max      3.574488    3.844715    3.574488  \n",
       "\n",
       "[8 rows x 901 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#statistical info of the data\n",
    "df_trainset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we started by verifying the existence of duplicated values, presented the target variable \"TenYearCHD\" distribution in the dataset, analyzed the correlations between the features and the target variable and in the end we also presented a plot with the 2 most correlated features with the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set duplicates: 23\n",
      "Test set duplicates: 0\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#We dont think duplicates are bad here, we just want to be aware of them\n",
    "\n",
    "#check for duplicates training set\n",
    "print(\"Train set duplicates:\",df_trainset.duplicated().sum())\n",
    "\n",
    "#check for duplicates test set\n",
    "print(\"Test set duplicates:\",df_testset.duplicated().sum())\n",
    "\n",
    "#Checking for nulls - there are none\n",
    "\n",
    "#check for null values in the entire train set dataframe\n",
    "print(df_trainset.isnull().any().any())\n",
    "\n",
    "#check for null values in the entire test set dataframe\n",
    "print(df_testset.isnull().any().any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Outliers\n",
    "Identify outliers and anomalies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers \n",
      "      class  t1        t2        t3        t4       t5        t6        t7  \\\n",
      "0      NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "1      NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "2      NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "3      NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "4      NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "..     ...  ..       ...       ...       ...      ...       ...       ...   \n",
      "176    NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "177    NaN NaN -3.719033 -3.731076 -3.715019 -3.75516 -3.751146 -3.803328   \n",
      "178    NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "179    NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "180    NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "\n",
      "           t8        t9  ...  t891  t892  t893      t894  t895  t896  t897  \\\n",
      "0         NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "1         NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "2         NaN       NaN  ...   NaN   NaN   NaN  4.024867   NaN   NaN   NaN   \n",
      "3         NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "4         NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "..        ...       ...  ...   ...   ...   ...       ...   ...   ...   ...   \n",
      "176       NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "177 -3.799314 -3.731076  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "178       NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "179       NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "180       NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "\n",
      "     t898  t899  t900  \n",
      "0     NaN   NaN   NaN  \n",
      "1     NaN   NaN   NaN  \n",
      "2     NaN   NaN   NaN  \n",
      "3     NaN   NaN   NaN  \n",
      "4     NaN   NaN   NaN  \n",
      "..    ...   ...   ...  \n",
      "176   NaN   NaN   NaN  \n",
      "177   NaN   NaN   NaN  \n",
      "178   NaN   NaN   NaN  \n",
      "179   NaN   NaN   NaN  \n",
      "180   NaN   NaN   NaN  \n",
      "\n",
      "[181 rows x 901 columns] \n",
      "\n",
      "outliers count \n",
      " 238 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFor now, vou esquever isto\\nVou normalizar o data set, e depois, se tivermos más classificações, volot a isto\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the z-score for each point of the training set\n",
    "z_scores = np.abs((df_trainset - df_trainset.mean()) / df_trainset.std())\n",
    "\n",
    "#define a threshold value\n",
    "threshold = 3 # its considered an outiler when the value of the point is 3 * mean of the training set, so the threshold is 3\n",
    "\n",
    "#Identify the outliers\n",
    "outliers = df_trainset[z_scores > threshold]\n",
    "\n",
    "#Count the number of outliers\n",
    "num_outliers = outliers.count().sum()\n",
    "\n",
    "\n",
    "print(f\"outliers \\n {outliers} \\n\") # non null values represent the outliers\n",
    "print(f\"outliers count \\n {num_outliers} \\n\")\n",
    "'''\n",
    "For now, vou esquever isto\n",
    "Vou normalizar o data set, e depois, se tivermos más classificações, volot a isto\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlations Matrix\n",
      "\n",
      "class    1.000000\n",
      "t56      0.183297\n",
      "t54      0.181402\n",
      "t55      0.177794\n",
      "t57      0.175693\n",
      "           ...   \n",
      "t219    -0.206140\n",
      "t216    -0.209983\n",
      "t215    -0.211973\n",
      "t218    -0.216237\n",
      "t217    -0.217713\n",
      "Name: class, Length: 901, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#creates a matrix of correlations\n",
    "corr_matrix = df_trainset.corr() \n",
    "#how much each attribute correlates with the Class target variable value, the lower the value the least relevant the feature is\n",
    "print(\"\\nCorrelations Matrix\\n\")\n",
    "print(corr_matrix['class'].sort_values(ascending=False))#to present all columns their type cannot be object so we must convert it to float\n",
    "\n",
    "#plot to present the correlation between the 2 most correlated features with target variable\n",
    "#sns.pairplot(df, vars=['Y9', 'Y10'], hue='Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "\n",
      "           t1        t2        t3        t4        t5        t6        t7  \\\n",
      "0    1.660505  1.739092  1.812766  1.847148  1.901176  1.935558  1.906088   \n",
      "1   -0.379133  0.242145 -0.517195 -0.033979  0.587299 -0.517195 -0.172040   \n",
      "2    0.534425  0.444349  0.399312  0.511906  0.669539  0.714577  0.511906   \n",
      "3   -2.438882 -2.412564 -2.438882 -2.333611 -2.267818 -2.307294 -2.412564   \n",
      "4    1.601259  1.601259  1.589440  1.589440  1.589440  1.589440  1.589440   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "176 -0.816431 -0.804662 -0.706590 -0.612441 -0.624210 -0.624210 -0.628132   \n",
      "177 -3.739104 -3.719033 -3.731076 -3.715019 -3.755160 -3.751146 -3.803328   \n",
      "178 -1.010301 -1.151468 -1.201885 -1.232135 -1.332969 -1.353136 -1.403553   \n",
      "179  1.511671  1.577663  1.569414  1.618907  1.618907  1.602410  1.635405   \n",
      "180  0.732443  0.698494  0.694251  0.673033  0.711225  0.723955  0.728199   \n",
      "\n",
      "           t8        t9       t10  ...      t891      t892      t893  \\\n",
      "0    1.945381  1.925734  1.915911  ... -0.363097 -0.422037 -0.397478   \n",
      "1    0.035052  0.518269 -0.103009  ... -3.485523 -3.623585 -2.311998   \n",
      "2    0.692058  0.489387  0.556944  ...  3.799677  3.934791  3.754639   \n",
      "3   -2.162547 -2.241500 -2.254659  ... -1.715148 -1.767783 -1.794101   \n",
      "4    1.577622  1.577622  1.577622  ...  3.445002  3.208625  3.244082   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "176 -0.635978 -0.518292 -0.518292  ... -1.769692 -1.757923 -1.734386   \n",
      "177 -3.799314 -3.731076 -3.807342  ...  0.186612  0.150486  0.126402   \n",
      "178 -1.383386 -1.393470 -1.393470  ...  1.702134  1.702134  1.712217   \n",
      "179  1.511671  1.552916  1.561165  ...  0.455808  0.505302  0.455808   \n",
      "180  0.766391  0.736686  0.783365  ... -0.472727 -0.485458 -0.472727   \n",
      "\n",
      "         t894      t895      t896      t897      t898      t899      t900  \n",
      "0   -0.402390 -0.402390 -0.402390 -0.441683 -0.431860 -0.495712 -0.505535  \n",
      "1   -3.278430 -2.864245 -3.278430 -3.002307 -2.864245 -2.726183 -3.071337  \n",
      "2    4.024867  3.799677  3.777158  3.777158  3.574488  3.844715  3.574488  \n",
      "3   -1.688830 -1.701989 -1.701989 -1.767783 -1.767783 -1.754624 -1.767783  \n",
      "4    3.125893  3.031342  2.948610  2.854059  2.783146  2.794965  2.854059  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "176 -1.714772 -1.714772 -1.718695 -1.726540 -1.734386 -1.742232 -1.730463  \n",
      "177  0.182598  0.190626  0.174570  0.210696  0.210696  0.210696  0.250836  \n",
      "178  1.712217  1.712217  1.722301  1.722301  1.732384  1.732384  1.732384  \n",
      "179  0.381568  0.406315  0.315577  0.257834  0.266083  0.183594  0.142349  \n",
      "180 -0.489702 -0.549111 -0.583060 -0.608521 -0.608521 -0.600034 -0.617008  \n",
      "\n",
      "[181 rows x 900 columns]\n",
      "\n",
      "y_train:\n",
      "\n",
      "0      1.0\n",
      "1      1.0\n",
      "2      1.0\n",
      "3      1.0\n",
      "4      1.0\n",
      "      ... \n",
      "176    2.0\n",
      "177    2.0\n",
      "178    2.0\n",
      "179    2.0\n",
      "180    2.0\n",
      "Name: class, Length: 181, dtype: float64\n",
      "\n",
      "X_test:\n",
      "\n",
      "          t1        t2        t3        t4        t5        t6        t7  \\\n",
      "0  -0.778589 -0.744436 -0.725462 -0.702693 -0.714077 -0.721667 -0.733052   \n",
      "1   0.017272 -0.071910 -0.083542 -0.091297 -0.106807 -0.149459 -0.215376   \n",
      "2   0.083460  0.074729  0.079095  0.083460  0.135847  0.179503  0.236255   \n",
      "3  -1.298560 -1.171976 -1.087587 -0.991141 -0.997169 -1.015253 -1.009225   \n",
      "4  -0.576294 -0.594890 -0.617206 -0.654399 -0.672995 -0.680433 -0.717626   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "72  1.306131  1.293378  1.306131  1.312507  1.318884  1.293378  1.287001   \n",
      "73 -1.592920 -1.592920 -1.592920 -1.592920 -1.592920 -1.519859 -1.446798   \n",
      "74 -4.539753 -4.346270 -4.447618 -4.355484 -4.198854 -4.189641 -4.088292   \n",
      "75 -3.739104 -3.719033 -3.731076 -3.715019 -3.755160 -3.751146 -3.803328   \n",
      "76 -0.816431 -0.804662 -0.706590 -0.612441 -0.624210 -0.624210 -0.628132   \n",
      "\n",
      "          t8        t9       t10  ...      t891      t892      t893      t894  \\\n",
      "0  -0.717872 -0.736846 -0.763410  ...  0.082832  0.052474 -0.042396 -0.099319   \n",
      "1  -0.211499 -0.199866 -0.215376  ...  0.668686  0.684196  0.672564  0.722971   \n",
      "2   0.319201  0.419610  0.437072  ...  0.476362  0.454534  0.458900  0.471997   \n",
      "3  -0.979086 -0.991141 -0.954975  ... -0.563166 -0.502888 -0.502888 -0.412471   \n",
      "4  -0.747380 -0.751099 -0.792011  ... -1.379653 -1.372214 -1.372214 -1.320145   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "72  1.261495  1.287001  1.280625  ... -1.257230 -1.257230 -1.257230 -1.263607   \n",
      "73 -1.428532 -1.392001 -1.428532  ... -0.241285 -0.350877 -0.296081 -0.131693   \n",
      "74 -3.996157 -3.894809 -3.811888  ...  1.310814  1.273960  1.264746  1.237106   \n",
      "75 -3.799314 -3.731076 -3.807342  ...  0.186612  0.150486  0.126402  0.182598   \n",
      "76 -0.635978 -0.518292 -0.518292  ... -1.769692 -1.757923 -1.734386 -1.714772   \n",
      "\n",
      "        t895      t896      t897      t898      t899      t900  \n",
      "0  -0.125882 -0.190394 -0.228342 -0.205573 -0.273880 -0.323212  \n",
      "1   0.742358  0.753991  0.781133  0.788888  0.796643  0.804398  \n",
      "2   0.489459  0.476362  0.489459  0.493824  0.489459  0.476362  \n",
      "3  -0.370276 -0.297942 -0.231636 -0.183413 -0.123135 -0.099024  \n",
      "4  -1.312706 -1.286672 -1.268075 -1.238321 -1.193690 -1.145340  \n",
      "..       ...       ...       ...       ...       ...       ...  \n",
      "72 -1.263607 -1.263607 -1.263607 -1.263607 -1.263607 -1.263607  \n",
      "73 -0.204754 -0.204754 -0.259550 -0.314346 -0.369143 -0.423939  \n",
      "74  1.218679  1.200252  1.181825  1.089690  0.988341  0.997555  \n",
      "75  0.190626  0.174570  0.210696  0.210696  0.210696  0.250836  \n",
      "76 -1.714772 -1.718695 -1.726540 -1.734386 -1.742232 -1.730463  \n",
      "\n",
      "[77 rows x 900 columns]\n",
      "\n",
      "y_test:\n",
      "\n",
      "0     1.0\n",
      "1     1.0\n",
      "2     1.0\n",
      "3     1.0\n",
      "4     1.0\n",
      "     ... \n",
      "72    2.0\n",
      "73    2.0\n",
      "74    2.0\n",
      "75    2.0\n",
      "76    2.0\n",
      "Name: class, Length: 77, dtype: float64\n",
      "(181, 900)\n"
     ]
    }
   ],
   "source": [
    "#Here, we split the data into X and y\n",
    "#y is the target variable 'class', and X is everything else\n",
    "\n",
    "y_train = df_trainset['class']\n",
    "#X_test = X_test.transpose()\n",
    "X_train = df_trainset.drop(['class'], axis=1)\n",
    "y_test = df_testset['class']\n",
    "#y_test = y_test.transpose()\n",
    "X_test = df_testset.drop(['class'], axis=1)\n",
    "\n",
    "print(\"X_train:\\n\")\n",
    "print(X_train)\n",
    "print(\"\\ny_train:\\n\")\n",
    "print(y_train)\n",
    "print(\"\\nX_test:\\n\")\n",
    "print(X_test)\n",
    "print(\"\\ny_test:\\n\")\n",
    "print(y_test)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model/Representation Method for Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNeighborsTimeSeriesClassifier model implements the k-nearest neighbor for time series. \n",
    "\n",
    "We have three possible metrics, as seen below in comments\n",
    "* 1-NN with Euclidean distance\n",
    "* 1-NN with DTW\n",
    "* 1-NN with SAX, in this case you need to set two other parameters: `n_segments` and `alphabet_size_avg`. The first parameter means the number of Piecewise Aggregate Approximation pieces to compute (start by fixing it at 16) and the latter is the number of SAX symbols to use (start by fixing it at 10). To fix these parameters, you need to use the parameter `metric_params` in the class of the classifier and provide a dictionary with the two parameters required.\n",
    "\n",
    "We are going to use the accuracy score (from scikit-learn) to compare the methods. Also, our data is already splitted in train and test set, so we don't need to worry about splitting our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Precision is:  0.5455\n",
      "The Recall is:  0.7273\n",
      "The F1 score is:  0.6234\n",
      "The Matthews correlation coefficient is:  0.2727\n",
      "\n",
      "This is the Confusion Matrix\n",
      "    0   1\n",
      "0  24   9\n",
      "1  20  24\n"
     ]
    }
   ],
   "source": [
    "#K nearest Neighbors for time series\n",
    "\n",
    "#c = KNeighborsTimeSeriesClassifier(n_neighbors = 3, metric = 'euclidean')#0.6103896103896104\n",
    "c = KNeighborsTimeSeriesClassifier(n_neighbors = 1, metric = 'dtw') #0.6233766233766234\n",
    "#dict = {'n_segments' : 16 , 'alphabet_size_avg': 10}\n",
    "#c = KNeighborsTimeSeriesClassifier(n_neighbors = 1, metric = 'sax', metric_params = dict)#0.5974025974025974\n",
    "#c = SVC(C=50,gamma='auto')\n",
    "#c  = RandomForestClassifier(n_estimators=12, random_state=0)\n",
    "#c = LogisticRegression(C = 0.01)\n",
    "#c = DecisionTreeClassifier(criterion = 'gini')\n",
    "#c = KNeighborsClassifier(n_neighbors = 5, algorithm = 'ball_tree',weights = 'distance')\n",
    "#c = GaussianNB()\n",
    "\n",
    "c.fit(X_train, y_train)\n",
    "preds = c.predict(X_test)\n",
    "\n",
    "#accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "#print(accuracy)\n",
    "\n",
    "precision = precision_score(y_test, preds)\n",
    "recall = recall_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds)\n",
    "mcc = matthews_corrcoef(y_test, preds)\n",
    "\n",
    "\n",
    "print(\"The Precision is: %7.4f\" % precision)\n",
    "print(\"The Recall is: %7.4f\" % recall)\n",
    "print(\"The F1 score is: %7.4f\" % f1)\n",
    "print(\"The Matthews correlation coefficient is: %7.4f\" % mcc)\n",
    "print()\n",
    "print(\"This is the Confusion Matrix\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, preds)))\n",
    "\n",
    "#Test randomForest , SVM, decision trees , KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to explore some representation methods, namely the Piecewise Aggregate Approximation (PAA) and the Symbolic Aggregate Approximation (SAX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute list of scalers, imputers,models and present the results \n",
    "def test_models (scalers, representation_models, models, X_train, y_train, X_test,y_test, show_rep_model, show_model):\n",
    "    results =[]\n",
    "    ct = 0\n",
    "    for name_scaler, scaler in scalers:\n",
    "        if show_rep_model:\n",
    "            for name_rep_method, rep_method in representation_models:\n",
    "                for name_mod, model in models:\n",
    "                    #scaling\n",
    "                    scaler.fit(X_train)\n",
    "                    Xt_train = scaler.transform(X_train)\n",
    "                    Xt_test  = scaler.transform(X_test)\n",
    "\n",
    "                    #representation methods\n",
    "                    rep_method.fit(Xt_train)\n",
    "                    Xt_train = rep_method.transform(Xt_train)\n",
    "                    Xt_test  = rep_method.transform(Xt_test)\n",
    "\n",
    "                    #len of the array\n",
    "                    if len(Xt_train.shape) == 2:\n",
    "                        model.fit(Xt_train, y_train)\n",
    "                        preds = model.predict(Xt_test)\n",
    "                    else:\n",
    "                        model.fit(Xt_train[:,:,0], y_train) #[:,:,0] to have only 2 dimensions\n",
    "                        preds = model.predict(Xt_test[:,:,0]) #PREDICTION\n",
    "\n",
    "                    #save results\n",
    "                    results = save_results (name_scaler, scaler, name_rep_method, rep_method, name_mod, model, results,y_test, preds, show_model)\n",
    "        else:\n",
    "            for name_mod, model in models:\n",
    "                    #scaling\n",
    "                    scaler.fit(X_train)\n",
    "                    Xt_train = scaler.transform(X_train)\n",
    "                    Xt_test  = scaler.transform(X_test)\n",
    "\n",
    "                    #len of the array\n",
    "                    if len(Xt_train.shape) == 2:\n",
    "                        model.fit(Xt_train, y_train)\n",
    "                        preds = model.predict(Xt_test)\n",
    "                    else:\n",
    "                        model.fit(Xt_train[:,:,0], y_train) #[:,:,0] to have only 2 dimensions\n",
    "                        preds = model.predict(Xt_test[:,:,0]) #PREDICTION\n",
    "\n",
    "                    #save results\n",
    "                    results = save_results (name_scaler, scaler, \"\", \"\", name_mod, model, results,y_test, preds, show_model)\n",
    "\n",
    "        #present model number\n",
    "        if show_model:\n",
    "            ct += 1\n",
    "            print(\"\\nModel %d\" % ct)\n",
    "\n",
    "    \n",
    "    results_sorted = sorted(results, key=lambda x: x[8], reverse=True) #f1 sorted decreasing\n",
    "    display_results(results_sorted, show_rep_model)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Save the model scores and present intermediate results (w/ show_model)\n",
    "# Returns the list with the saved results \n",
    "def save_results(name_scaler, scaler,name_rep_method, rep_method, name_mod, model, results, y_test, preds, show_model):\n",
    "\n",
    "    # Calculate the precision, recall, f1 and mcc scores\n",
    "    precision = precision_score(y_test, preds)\n",
    "    recall = recall_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    mcc = matthews_corrcoef(y_test, preds)\n",
    "    \n",
    "    if show_model:\n",
    "        print(f\"Scaler: {scaler} rep method: {rep_method} classifier: {name_mod} {model}\")\n",
    "        print(\"The Precision is: %7.4f\" % precision)\n",
    "        print(\"The Recall is: %7.4f\" % recall)\n",
    "        print(\"The F1 score is: %7.4f\" % f1)\n",
    "        print(\"The Matthews correlation coefficient is: %7.4f\" % mcc)\n",
    "        print()\n",
    "        print(\"This is the Confusion Matrix\")\n",
    "        print(pd.DataFrame(confusion_matrix(y_test, preds)))\n",
    "\n",
    "\n",
    "    results.append((name_scaler,\n",
    "                    scaler,\n",
    "                    name_rep_method, \n",
    "                    rep_method, \n",
    "                    name_mod, \n",
    "                    model,\n",
    "                    precision,\n",
    "                    recall,\n",
    "                    f1,\n",
    "                    mcc,                    \n",
    "                    ))\n",
    "    return results\n",
    "\n",
    "# Display the model final results. Receives the ordered results to present\n",
    "def display_results (results, show_rep_model):        \n",
    "    \n",
    "    noshow = \"\"\n",
    "    if show_rep_model:\n",
    "        print (f\"\\n--------------------------Results for Representation Methods Performance--------------------------\")\n",
    "    else:\n",
    "        print (f\"\\n--------------------------Results for Classification Models Performance--------------------------\")\n",
    "    for res in results:\n",
    "        name_scaler = res [0]\n",
    "        scaler = res [1]\n",
    "        name_rep_method = res [2]\n",
    "        rep_method = res [3]\n",
    "        name_mod = res [4]\n",
    "        model = res [5]\n",
    "        precision = res [6]\n",
    "        recall = res [7]\n",
    "        f1 = res [8]\n",
    "        mcc = res [9]\n",
    "\n",
    "        if show_rep_model:\n",
    "            print(f\"{name_mod.ljust(25)} | precision     {precision:.4f} | recall     {recall:.4f} | f1     {f1:.4f}| mcc     {mcc:.4f}\")\n",
    "            print(f\"{noshow.ljust(25)} | scaler {scaler} | rep method {rep_method}\")\n",
    "        else:\n",
    "             print(f\"{name_mod.ljust(25)} | precision     {precision:.4f} | recall     {recall:.4f} | f1     {f1:.4f}| mcc     {mcc:.4f}\")\n",
    "             print(f\"{noshow.ljust(25)} | scaler {scaler}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------Results for Classification Models Performance--------------------------\n",
      "SVC_c10                   | precision     0.5750 | recall     0.6970 | f1     0.6301| mcc     0.3077\n",
      "                          | scaler PowerTransformer()\n",
      "SVC_c10                   | precision     0.5750 | recall     0.6970 | f1     0.6301| mcc     0.3077\n",
      "                          | scaler StandardScaler()\n",
      "KNNTM_K1_dtw              | precision     0.5455 | recall     0.7273 | f1     0.6234| mcc     0.2727\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "KNNTM_K1_dtw              | precision     0.5610 | recall     0.6970 | f1     0.6216| mcc     0.2855\n",
      "                          | scaler StandardScaler()\n",
      "SVC_c10                   | precision     0.5500 | recall     0.6667 | f1     0.6027| mcc     0.2551\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "KNNTM_K1_sax              | precision     0.4286 | recall     1.0000 | f1     0.6000| mcc     0.0000\n",
      "                          | scaler MinMaxScaler()\n",
      "RandomForestClassifier_ne10 | precision     0.5714 | recall     0.6061 | f1     0.5882| mcc     0.2635\n",
      "                          | scaler MinMaxScaler()\n",
      "RandomForestClassifier_ne10 | precision     0.5714 | recall     0.6061 | f1     0.5882| mcc     0.2635\n",
      "                          | scaler StandardScaler()\n",
      "RandomForestClassifier_ne10 | precision     0.5714 | recall     0.6061 | f1     0.5882| mcc     0.2635\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "SVC_c50                   | precision     0.5385 | recall     0.6364 | f1     0.5833| mcc     0.2250\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "RandomForestClassifier_ne10 | precision     0.5556 | recall     0.6061 | f1     0.5797| mcc     0.2404\n",
      "                          | scaler PowerTransformer()\n",
      "KNNTM_K1_dtw              | precision     0.5116 | recall     0.6667 | f1     0.5789| mcc     0.1887\n",
      "                          | scaler PowerTransformer()\n",
      "SVC_c50                   | precision     0.5250 | recall     0.6364 | f1     0.5753| mcc     0.2026\n",
      "                          | scaler PowerTransformer()\n",
      "SVC_c50                   | precision     0.5250 | recall     0.6364 | f1     0.5753| mcc     0.2026\n",
      "                          | scaler StandardScaler()\n",
      "KNN_K1_wdist              | precision     0.5806 | recall     0.5455 | f1     0.5625| mcc     0.2523\n",
      "                          | scaler PowerTransformer()\n",
      "KNNTM_K1_eu               | precision     0.5806 | recall     0.5455 | f1     0.5625| mcc     0.2523\n",
      "                          | scaler PowerTransformer()\n",
      "KNN_K1_wdist              | precision     0.5625 | recall     0.5455 | f1     0.5538| mcc     0.2282\n",
      "                          | scaler StandardScaler()\n",
      "KNNTM_K1_eu               | precision     0.5625 | recall     0.5455 | f1     0.5538| mcc     0.2282\n",
      "                          | scaler StandardScaler()\n",
      "DecisionTree_critgini     | precision     0.5926 | recall     0.4848 | f1     0.5333| mcc     0.2436\n",
      "                          | scaler StandardScaler()\n",
      "DecisionTree_maxd10       | precision     0.5484 | recall     0.5152 | f1     0.5312| mcc     0.1988\n",
      "                          | scaler MinMaxScaler()\n",
      "KNN_K1_wdist              | precision     0.5484 | recall     0.5152 | f1     0.5312| mcc     0.1988\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "KNNTM_K1_eu               | precision     0.5484 | recall     0.5152 | f1     0.5312| mcc     0.1988\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "DecisionTree_critentropy  | precision     0.5312 | recall     0.5152 | f1     0.5231| mcc     0.1750\n",
      "                          | scaler PowerTransformer()\n",
      "LogisticRegression        | precision     0.5000 | recall     0.5455 | f1     0.5217| mcc     0.1353\n",
      "                          | scaler StandardScaler()\n",
      "DecisionTree_maxd10       | precision     0.5517 | recall     0.4848 | f1     0.5161| mcc     0.1934\n",
      "                          | scaler PowerTransformer()\n",
      "KNN_K1_wdist              | precision     0.5333 | recall     0.4848 | f1     0.5079| mcc     0.1691\n",
      "                          | scaler MinMaxScaler()\n",
      "KNNTM_K1_eu               | precision     0.5333 | recall     0.4848 | f1     0.5079| mcc     0.1691\n",
      "                          | scaler MinMaxScaler()\n",
      "DecisionTree_critentropy  | precision     0.5333 | recall     0.4848 | f1     0.5079| mcc     0.1691\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "LogisticRegression        | precision     0.4737 | recall     0.5455 | f1     0.5070| mcc     0.0900\n",
      "                          | scaler PowerTransformer()\n",
      "DecisionTree_critgini     | precision     0.5556 | recall     0.4545 | f1     0.5000| mcc     0.1886\n",
      "                          | scaler PowerTransformer()\n",
      "KNNTM_K1_dtw              | precision     0.5000 | recall     0.4848 | f1     0.4923| mcc     0.1217\n",
      "                          | scaler MinMaxScaler()\n",
      "DecisionTree_maxd10       | precision     0.5357 | recall     0.4545 | f1     0.4918| mcc     0.1637\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "DecisionTree_critgini     | precision     0.5357 | recall     0.4545 | f1     0.4918| mcc     0.1637\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "SVC_c50                   | precision     0.4595 | recall     0.5152 | f1     0.4857| mcc     0.0600\n",
      "                          | scaler MinMaxScaler()\n",
      "LogisticRegression        | precision     0.4848 | recall     0.4848 | f1     0.4848| mcc     0.0985\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "GaussianNB                | precision     0.4474 | recall     0.5152 | f1     0.4789| mcc     0.0375\n",
      "                          | scaler PowerTransformer()\n",
      "GaussianNB                | precision     0.4474 | recall     0.5152 | f1     0.4789| mcc     0.0375\n",
      "                          | scaler MinMaxScaler()\n",
      "GaussianNB                | precision     0.4474 | recall     0.5152 | f1     0.4789| mcc     0.0375\n",
      "                          | scaler StandardScaler()\n",
      "GaussianNB                | precision     0.4474 | recall     0.5152 | f1     0.4789| mcc     0.0375\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "DecisionTree_minsl20      | precision     0.4706 | recall     0.4848 | f1     0.4776| mcc     0.0755\n",
      "                          | scaler MinMaxScaler()\n",
      "DecisionTree_critgini     | precision     0.5000 | recall     0.4545 | f1     0.4762| mcc     0.1153\n",
      "                          | scaler MinMaxScaler()\n",
      "DecisionTree_critentropy  | precision     0.5385 | recall     0.4242 | f1     0.4746| mcc     0.1586\n",
      "                          | scaler StandardScaler()\n",
      "DecisionTree_minsl20      | precision     0.4571 | recall     0.4848 | f1     0.4706| mcc     0.0527\n",
      "                          | scaler PowerTransformer()\n",
      "DecisionTree_minsl20      | precision     0.4571 | recall     0.4848 | f1     0.4706| mcc     0.0527\n",
      "                          | scaler StandardScaler()\n",
      "RandomForestClassifier_ne50 | precision     0.5185 | recall     0.4242 | f1     0.4667| mcc     0.1336\n",
      "                          | scaler PowerTransformer()\n",
      "RandomForestClassifier_ne50 | precision     0.5185 | recall     0.4242 | f1     0.4667| mcc     0.1336\n",
      "                          | scaler MinMaxScaler()\n",
      "RandomForestClassifier_ne50 | precision     0.5185 | recall     0.4242 | f1     0.4667| mcc     0.1336\n",
      "                          | scaler StandardScaler()\n",
      "RandomForestClassifier_ne50 | precision     0.5185 | recall     0.4242 | f1     0.4667| mcc     0.1336\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "KNNTM_K1_sax              | precision     0.5417 | recall     0.3939 | f1     0.4561| mcc     0.1538\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "DecisionTree_maxd10       | precision     0.4828 | recall     0.4242 | f1     0.4516| mcc     0.0851\n",
      "                          | scaler StandardScaler()\n",
      "KNNTM_K1_sax              | precision     0.5455 | recall     0.3636 | f1     0.4364| mcc     0.1494\n",
      "                          | scaler PowerTransformer()\n",
      "KNNTM_K1_sax              | precision     0.5455 | recall     0.3636 | f1     0.4364| mcc     0.1494\n",
      "                          | scaler StandardScaler()\n",
      "DecisionTree_critentropy  | precision     0.4483 | recall     0.3939 | f1     0.4194| mcc     0.0309\n",
      "                          | scaler MinMaxScaler()\n",
      "DecisionTree_minsl20      | precision     0.4194 | recall     0.3939 | f1     0.4062| mcc     -0.0153\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
      "SVC_c10                   | precision     0.4545 | recall     0.3030 | f1     0.3636| mcc     0.0332\n",
      "                          | scaler MinMaxScaler()\n",
      "LogisticRegression        | precision     0.5000 | recall     0.1515 | f1     0.2326| mcc     0.0558\n",
      "                          | scaler MinMaxScaler()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\piecewise\\piecewise.py:152: RuntimeWarning: Mean of empty slice.\n",
      "  X_transformed[i_ts, i_seg, :] = segment.mean(axis=0)\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------Results for Representation Methods Performance--------------------------\n",
      "KNNTM_K1_dtw              | precision     0.6279 | recall     0.8182 | f1     0.7105| mcc     0.4530\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_dtw              | precision     0.5854 | recall     0.7273 | f1     0.6486| mcc     0.3381\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_critentropy  | precision     0.5854 | recall     0.7273 | f1     0.6486| mcc     0.3381\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "SVC_c50                   | precision     0.5854 | recall     0.7273 | f1     0.6486| mcc     0.3381\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_maxd10       | precision     0.6286 | recall     0.6667 | f1     0.6471| mcc     0.3689\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_minsl20      | precision     0.5581 | recall     0.7273 | f1     0.6316| mcc     0.2944\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "SVC_c50                   | precision     0.5750 | recall     0.6970 | f1     0.6301| mcc     0.3077\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_dtw              | precision     0.5946 | recall     0.6667 | f1     0.6286| mcc     0.3227\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "SVC_c50                   | precision     0.5789 | recall     0.6667 | f1     0.6197| mcc     0.2999\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "SVC_c10                   | precision     0.5789 | recall     0.6667 | f1     0.6197| mcc     0.2999\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_dtw              | precision     0.5641 | recall     0.6667 | f1     0.6111| mcc     0.2775\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_dtw              | precision     0.5641 | recall     0.6667 | f1     0.6111| mcc     0.2775\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "SVC_c10                   | precision     0.5833 | recall     0.6364 | f1     0.6087| mcc     0.2930\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_dtw              | precision     0.5833 | recall     0.6364 | f1     0.6087| mcc     0.2930\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_dtw              | precision     0.6061 | recall     0.6061 | f1     0.6061| mcc     0.3106\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_critentropy  | precision     0.6061 | recall     0.6061 | f1     0.6061| mcc     0.3106\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_eu               | precision     0.6061 | recall     0.6061 | f1     0.6061| mcc     0.3106\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_maxd10       | precision     0.6333 | recall     0.5758 | f1     0.6032| mcc     0.3306\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne10 | precision     0.6333 | recall     0.5758 | f1     0.6032| mcc     0.3306\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_maxd10       | precision     0.5500 | recall     0.6667 | f1     0.6027| mcc     0.2551\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "RandomForestClassifier_ne10 | precision     0.5676 | recall     0.6364 | f1     0.6000| mcc     0.2701\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "SVC_c50                   | precision     0.5676 | recall     0.6364 | f1     0.6000| mcc     0.2701\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_sax              | precision     0.4286 | recall     1.0000 | f1     0.6000| mcc     0.0000\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_sax              | precision     0.4286 | recall     1.0000 | f1     0.6000| mcc     0.0000\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNNTM_K1_sax              | precision     0.4286 | recall     1.0000 | f1     0.6000| mcc     0.0000\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_sax              | precision     0.4286 | recall     1.0000 | f1     0.6000| mcc     0.0000\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_sax              | precision     0.4286 | recall     1.0000 | f1     0.6000| mcc     0.0000\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNNTM_K1_sax              | precision     0.4286 | recall     1.0000 | f1     0.6000| mcc     0.0000\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNNTM_K1_sax              | precision     0.4286 | recall     1.0000 | f1     0.6000| mcc     0.0000\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_sax              | precision     0.4286 | recall     1.0000 | f1     0.6000| mcc     0.0000\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNNTM_K1_sax              | precision     0.4286 | recall     1.0000 | f1     0.6000| mcc     0.0000\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_sax              | precision     0.4286 | recall     1.0000 | f1     0.6000| mcc     0.0000\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNN_K1_wdist              | precision     0.6129 | recall     0.5758 | f1     0.5938| mcc     0.3058\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNNTM_K1_eu               | precision     0.6129 | recall     0.5758 | f1     0.5938| mcc     0.3058\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "SVC_c10                   | precision     0.5526 | recall     0.6364 | f1     0.5915| mcc     0.2475\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "SVC_c50                   | precision     0.5714 | recall     0.6061 | f1     0.5882| mcc     0.2635\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne10 | precision     0.5714 | recall     0.6061 | f1     0.5882| mcc     0.2635\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNN_K1_wdist              | precision     0.5938 | recall     0.5758 | f1     0.5846| mcc     0.2815\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_eu               | precision     0.5938 | recall     0.5758 | f1     0.5846| mcc     0.2815\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNN_K1_wdist              | precision     0.5938 | recall     0.5758 | f1     0.5846| mcc     0.2815\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_critgini     | precision     0.5385 | recall     0.6364 | f1     0.5833| mcc     0.2250\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "SVC_c50                   | precision     0.5385 | recall     0.6364 | f1     0.5833| mcc     0.2250\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne10 | precision     0.6207 | recall     0.5455 | f1     0.5806| mcc     0.3018\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNN_K1_wdist              | precision     0.5556 | recall     0.6061 | f1     0.5797| mcc     0.2404\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNNTM_K1_eu               | precision     0.5556 | recall     0.6061 | f1     0.5797| mcc     0.2404\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "SVC_c10                   | precision     0.5556 | recall     0.6061 | f1     0.5797| mcc     0.2404\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_dtw              | precision     0.5556 | recall     0.6061 | f1     0.5797| mcc     0.2404\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNNTM_K1_sax              | precision     0.4189 | recall     0.9394 | f1     0.5794| mcc     -0.0969\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNNTM_K1_sax              | precision     0.4189 | recall     0.9394 | f1     0.5794| mcc     -0.0969\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNNTM_K1_sax              | precision     0.4133 | recall     0.9394 | f1     0.5741| mcc     -0.1886\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "SVC_c10                   | precision     0.5405 | recall     0.6061 | f1     0.5714| mcc     0.2176\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_maxd10       | precision     0.5405 | recall     0.6061 | f1     0.5714| mcc     0.2176\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_critgini     | precision     0.5405 | recall     0.6061 | f1     0.5714| mcc     0.2176\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "SVC_c10                   | precision     0.5405 | recall     0.6061 | f1     0.5714| mcc     0.2176\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_critgini     | precision     0.5405 | recall     0.6061 | f1     0.5714| mcc     0.2176\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "RandomForestClassifier_ne10 | precision     0.5588 | recall     0.5758 | f1     0.5672| mcc     0.2340\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_critentropy  | precision     0.5588 | recall     0.5758 | f1     0.5672| mcc     0.2340\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNN_K1_wdist              | precision     0.6296 | recall     0.5152 | f1     0.5667| mcc     0.2986\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNNTM_K1_eu               | precision     0.6296 | recall     0.5152 | f1     0.5667| mcc     0.2986\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNN_K1_wdist              | precision     0.6296 | recall     0.5152 | f1     0.5667| mcc     0.2986\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNNTM_K1_eu               | precision     0.6296 | recall     0.5152 | f1     0.5667| mcc     0.2986\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_maxd10       | precision     0.5806 | recall     0.5455 | f1     0.5625| mcc     0.2523\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNN_K1_wdist              | precision     0.5806 | recall     0.5455 | f1     0.5625| mcc     0.2523\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_eu               | precision     0.5806 | recall     0.5455 | f1     0.5625| mcc     0.2523\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne50 | precision     0.6667 | recall     0.4848 | f1     0.5614| mcc     0.3238\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "RandomForestClassifier_ne50 | precision     0.6071 | recall     0.5152 | f1     0.5574| mcc     0.2728\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne50 | precision     0.6071 | recall     0.5152 | f1     0.5574| mcc     0.2728\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_critentropy  | precision     0.6071 | recall     0.5152 | f1     0.5574| mcc     0.2728\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNNTM_K1_dtw              | precision     0.6071 | recall     0.5152 | f1     0.5574| mcc     0.2728\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNNTM_K1_dtw              | precision     0.7143 | recall     0.4545 | f1     0.5556| mcc     0.3536\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "RandomForestClassifier_ne10 | precision     0.5625 | recall     0.5455 | f1     0.5538| mcc     0.2282\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "RandomForestClassifier_ne10 | precision     0.5625 | recall     0.5455 | f1     0.5538| mcc     0.2282\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_maxd10       | precision     0.5625 | recall     0.5455 | f1     0.5538| mcc     0.2282\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNNTM_K1_dtw              | precision     0.5862 | recall     0.5152 | f1     0.5484| mcc     0.2476\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "RandomForestClassifier_ne10 | precision     0.5862 | recall     0.5152 | f1     0.5484| mcc     0.2476\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNN_K1_wdist              | precision     0.5455 | recall     0.5455 | f1     0.5455| mcc     0.2045\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_eu               | precision     0.5455 | recall     0.5455 | f1     0.5455| mcc     0.2045\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_critentropy  | precision     0.5455 | recall     0.5455 | f1     0.5455| mcc     0.2045\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_minsl20      | precision     0.5455 | recall     0.5455 | f1     0.5455| mcc     0.2045\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne50 | precision     0.5455 | recall     0.5455 | f1     0.5455| mcc     0.2045\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "LogisticRegression        | precision     0.5455 | recall     0.5455 | f1     0.5455| mcc     0.2045\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_critgini     | precision     0.5135 | recall     0.5758 | f1     0.5429| mcc     0.1651\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_dtw              | precision     0.4878 | recall     0.6061 | f1     0.5405| mcc     0.1277\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne10 | precision     0.5667 | recall     0.5152 | f1     0.5397| mcc     0.2229\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_maxd10       | precision     0.5294 | recall     0.5455 | f1     0.5373| mcc     0.1812\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_dtw              | precision     0.4762 | recall     0.6061 | f1     0.5333| mcc     0.1054\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne50 | precision     0.5926 | recall     0.4848 | f1     0.5333| mcc     0.2436\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne50 | precision     0.5926 | recall     0.4848 | f1     0.5333| mcc     0.2436\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNN_K1_wdist              | precision     0.5926 | recall     0.4848 | f1     0.5333| mcc     0.2436\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNNTM_K1_eu               | precision     0.5926 | recall     0.4848 | f1     0.5333| mcc     0.2436\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "RandomForestClassifier_ne50 | precision     0.5484 | recall     0.5152 | f1     0.5312| mcc     0.1988\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_critgini     | precision     0.5484 | recall     0.5152 | f1     0.5312| mcc     0.1988\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNNTM_K1_dtw              | precision     0.5143 | recall     0.5455 | f1     0.5294| mcc     0.1581\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "RandomForestClassifier_ne10 | precision     0.5143 | recall     0.5455 | f1     0.5294| mcc     0.1581\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNN_K1_wdist              | precision     0.5714 | recall     0.4848 | f1     0.5246| mcc     0.2182\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNNTM_K1_eu               | precision     0.5714 | recall     0.4848 | f1     0.5246| mcc     0.2182\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "RandomForestClassifier_ne50 | precision     0.5714 | recall     0.4848 | f1     0.5246| mcc     0.2182\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_critgini     | precision     0.5312 | recall     0.5152 | f1     0.5231| mcc     0.1750\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNN_K1_wdist              | precision     0.5312 | recall     0.5152 | f1     0.5231| mcc     0.1750\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_eu               | precision     0.5312 | recall     0.5152 | f1     0.5231| mcc     0.1750\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_maxd10       | precision     0.5312 | recall     0.5152 | f1     0.5231| mcc     0.1750\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_critgini     | precision     0.5000 | recall     0.5455 | f1     0.5217| mcc     0.1353\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "RandomForestClassifier_ne10 | precision     0.5000 | recall     0.5455 | f1     0.5217| mcc     0.1353\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "RandomForestClassifier_ne50 | precision     0.6667 | recall     0.4242 | f1     0.5185| mcc     0.2946\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_critgini     | precision     0.6000 | recall     0.4545 | f1     0.5172| mcc     0.2402\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_critgini     | precision     0.5517 | recall     0.4848 | f1     0.5161| mcc     0.1934\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne10 | precision     0.5517 | recall     0.4848 | f1     0.5161| mcc     0.1934\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_dtw              | precision     0.5152 | recall     0.5152 | f1     0.5152| mcc     0.1515\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "RandomForestClassifier_ne10 | precision     0.5152 | recall     0.5152 | f1     0.5152| mcc     0.1515\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "LogisticRegression        | precision     0.5152 | recall     0.5152 | f1     0.5152| mcc     0.1515\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "RandomForestClassifier_ne10 | precision     0.4865 | recall     0.5455 | f1     0.5143| mcc     0.1126\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNNTM_K1_dtw              | precision     0.4634 | recall     0.5758 | f1     0.5135| mcc     0.0751\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "SVC_c50                   | precision     0.5769 | recall     0.4545 | f1     0.5085| mcc     0.2140\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne50 | precision     0.5769 | recall     0.4545 | f1     0.5085| mcc     0.2140\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "RandomForestClassifier_ne50 | precision     0.5769 | recall     0.4545 | f1     0.5085| mcc     0.2140\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_critentropy  | precision     0.5333 | recall     0.4848 | f1     0.5079| mcc     0.1691\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_maxd10       | precision     0.5333 | recall     0.4848 | f1     0.5079| mcc     0.1691\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_critentropy  | precision     0.5333 | recall     0.4848 | f1     0.5079| mcc     0.1691\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "RandomForestClassifier_ne10 | precision     0.4857 | recall     0.5152 | f1     0.5000| mcc     0.1054\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_critentropy  | precision     0.4857 | recall     0.5152 | f1     0.5000| mcc     0.1054\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_critentropy  | precision     0.6087 | recall     0.4242 | f1     0.5000| mcc     0.2375\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_maxd10       | precision     0.5161 | recall     0.4848 | f1     0.5000| mcc     0.1452\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "LogisticRegression        | precision     0.4857 | recall     0.5152 | f1     0.5000| mcc     0.1054\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_minsl20      | precision     0.5000 | recall     0.4848 | f1     0.4923| mcc     0.1217\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_maxd10       | precision     0.5000 | recall     0.4848 | f1     0.4923| mcc     0.1217\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_minsl20      | precision     0.5172 | recall     0.4545 | f1     0.4839| mcc     0.1393\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "SVC_c50                   | precision     0.5172 | recall     0.4545 | f1     0.4839| mcc     0.1393\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "GaussianNB                | precision     0.5172 | recall     0.4545 | f1     0.4839| mcc     0.1393\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNN_K1_wdist              | precision     0.5600 | recall     0.4242 | f1     0.4828| mcc     0.1841\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "KNNTM_K1_eu               | precision     0.5600 | recall     0.4242 | f1     0.4828| mcc     0.1841\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_maxd10       | precision     0.5600 | recall     0.4242 | f1     0.4828| mcc     0.1841\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_maxd10       | precision     0.4706 | recall     0.4848 | f1     0.4776| mcc     0.0755\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_critgini     | precision     0.4706 | recall     0.4848 | f1     0.4776| mcc     0.0755\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "GaussianNB                | precision     0.4706 | recall     0.4848 | f1     0.4776| mcc     0.0755\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "LogisticRegression        | precision     0.5385 | recall     0.4242 | f1     0.4746| mcc     0.1586\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_critgini     | precision     0.5385 | recall     0.4242 | f1     0.4746| mcc     0.1586\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "LogisticRegression        | precision     0.5385 | recall     0.4242 | f1     0.4746| mcc     0.1586\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_critgini     | precision     0.5385 | recall     0.4242 | f1     0.4746| mcc     0.1586\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "RandomForestClassifier_ne50 | precision     0.5385 | recall     0.4242 | f1     0.4746| mcc     0.1586\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "SVC_c50                   | precision     0.6667 | recall     0.3636 | f1     0.4706| mcc     0.2657\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "SVC_c10                   | precision     0.6667 | recall     0.3636 | f1     0.4706| mcc     0.2657\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_critentropy  | precision     0.4839 | recall     0.4545 | f1     0.4687| mcc     0.0917\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_critgini     | precision     0.4839 | recall     0.4545 | f1     0.4687| mcc     0.0917\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "RandomForestClassifier_ne50 | precision     0.5652 | recall     0.3939 | f1     0.4643| mcc     0.1802\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_minsl20      | precision     0.4444 | recall     0.4848 | f1     0.4638| mcc     0.0301\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "RandomForestClassifier_ne10 | precision     0.5000 | recall     0.4242 | f1     0.4590| mcc     0.1091\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_critentropy  | precision     0.5000 | recall     0.4242 | f1     0.4590| mcc     0.1091\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_minsl20      | precision     0.5000 | recall     0.4242 | f1     0.4590| mcc     0.1091\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "LogisticRegression        | precision     0.5000 | recall     0.4242 | f1     0.4590| mcc     0.1091\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_minsl20      | precision     0.4324 | recall     0.4848 | f1     0.4571| mcc     0.0075\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNN_K1_wdist              | precision     0.5417 | recall     0.3939 | f1     0.4561| mcc     0.1538\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_eu               | precision     0.5417 | recall     0.3939 | f1     0.4561| mcc     0.1538\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "LogisticRegression        | precision     0.5417 | recall     0.3939 | f1     0.4561| mcc     0.1538\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNNTM_K1_sax              | precision     0.5417 | recall     0.3939 | f1     0.4561| mcc     0.1538\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_maxd10       | precision     0.4545 | recall     0.4545 | f1     0.4545| mcc     0.0455\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_critgini     | precision     0.4545 | recall     0.4545 | f1     0.4545| mcc     0.0455\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "GaussianNB                | precision     0.4545 | recall     0.4545 | f1     0.4545| mcc     0.0455\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_critgini     | precision     0.4545 | recall     0.4545 | f1     0.4545| mcc     0.0455\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "RandomForestClassifier_ne50 | precision     0.6000 | recall     0.3636 | f1     0.4528| mcc     0.2052\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_critgini     | precision     0.6000 | recall     0.3636 | f1     0.4528| mcc     0.2052\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "GaussianNB                | precision     0.4828 | recall     0.4242 | f1     0.4516| mcc     0.0851\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_minsl20      | precision     0.5200 | recall     0.3939 | f1     0.4483| mcc     0.1281\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_maxd10       | precision     0.5200 | recall     0.3939 | f1     0.4483| mcc     0.1281\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "RandomForestClassifier_ne50 | precision     0.5200 | recall     0.3939 | f1     0.4483| mcc     0.1281\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_minsl20      | precision     0.4286 | recall     0.4545 | f1     0.4412| mcc     0.0000\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_critentropy  | precision     0.5000 | recall     0.3939 | f1     0.4407| mcc     0.1031\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "GaussianNB                | precision     0.5000 | recall     0.3939 | f1     0.4407| mcc     0.1031\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_maxd10       | precision     0.4516 | recall     0.4242 | f1     0.4375| mcc     0.0382\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "KNNTM_K1_sax              | precision     0.5455 | recall     0.3636 | f1     0.4364| mcc     0.1494\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_sax              | precision     0.5455 | recall     0.3636 | f1     0.4364| mcc     0.1494\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "RandomForestClassifier_ne50 | precision     0.5455 | recall     0.3636 | f1     0.4364| mcc     0.1494\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_minsl20      | precision     0.4167 | recall     0.4545 | f1     0.4348| mcc     -0.0225\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_critentropy  | precision     0.4815 | recall     0.3939 | f1     0.4333| mcc     0.0786\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "GaussianNB                | precision     0.4375 | recall     0.4242 | f1     0.4308| mcc     0.0152\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "GaussianNB                | precision     0.4643 | recall     0.3939 | f1     0.4262| mcc     0.0546\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "GaussianNB                | precision     0.4643 | recall     0.3939 | f1     0.4262| mcc     0.0546\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "GaussianNB                | precision     0.4643 | recall     0.3939 | f1     0.4262| mcc     0.0546\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "KNN_K1_wdist              | precision     0.5000 | recall     0.3636 | f1     0.4211| mcc     0.0971\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_eu               | precision     0.5000 | recall     0.3636 | f1     0.4211| mcc     0.0971\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "GaussianNB                | precision     0.4483 | recall     0.3939 | f1     0.4194| mcc     0.0309\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "GaussianNB                | precision     0.4483 | recall     0.3939 | f1     0.4194| mcc     0.0309\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "GaussianNB                | precision     0.4483 | recall     0.3939 | f1     0.4194| mcc     0.0309\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "GaussianNB                | precision     0.4800 | recall     0.3636 | f1     0.4138| mcc     0.0721\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "GaussianNB                | precision     0.4333 | recall     0.3939 | f1     0.4127| mcc     0.0077\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "DecisionTree_minsl20      | precision     0.4333 | recall     0.3939 | f1     0.4127| mcc     0.0077\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_minsl20      | precision     0.4000 | recall     0.4242 | f1     0.4118| mcc     -0.0527\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "GaussianNB                | precision     0.4615 | recall     0.3636 | f1     0.4068| mcc     0.0476\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_minsl20      | precision     0.3784 | recall     0.4242 | f1     0.4000| mcc     -0.0975\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_minsl20      | precision     0.3939 | recall     0.3939 | f1     0.3939| mcc     -0.0606\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_critentropy  | precision     0.4783 | recall     0.3333 | f1     0.3929| mcc     0.0655\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNN_K1_wdist              | precision     0.4783 | recall     0.3333 | f1     0.3929| mcc     0.0655\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_eu               | precision     0.4783 | recall     0.3333 | f1     0.3929| mcc     0.0655\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNN_K1_wdist              | precision     0.4583 | recall     0.3333 | f1     0.3860| mcc     0.0405\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "KNNTM_K1_eu               | precision     0.4583 | recall     0.3333 | f1     0.3860| mcc     0.0405\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "DecisionTree_critentropy  | precision     0.3871 | recall     0.3636 | f1     0.3750| mcc     -0.0688\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "GaussianNB                | precision     0.4074 | recall     0.3333 | f1     0.3667| mcc     -0.0314\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_minsl20      | precision     0.3929 | recall     0.3333 | f1     0.3607| mcc     -0.0546\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "DecisionTree_critentropy  | precision     0.3333 | recall     0.2727 | f1     0.3000| mcc     -0.1414\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "DecisionTree_minsl20      | precision     0.3200 | recall     0.2424 | f1     0.2759| mcc     -0.1521\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "SVC_c10                   | precision     0.7143 | recall     0.1515 | f1     0.2500| mcc     0.1826\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "SVC_c10                   | precision     0.3636 | recall     0.1212 | f1     0.1818| mcc     -0.0536\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "LogisticRegression        | precision     0.3077 | recall     0.1212 | f1     0.1739| mcc     -0.1101\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "LogisticRegression        | precision     0.2500 | recall     0.0303 | f1     0.0541| mcc     -0.0845\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "SVC_c50                   | precision     0.2500 | recall     0.0303 | f1     0.0541| mcc     -0.0845\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "SVC_c10                   | precision     0.2500 | recall     0.0303 | f1     0.0541| mcc     -0.0845\n",
      "                          | scaler MinMaxScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "LogisticRegression        | precision     0.2500 | recall     0.0303 | f1     0.0541| mcc     -0.0845\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "LogisticRegression        | precision     0.2500 | recall     0.0303 | f1     0.0541| mcc     -0.0845\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "LogisticRegression        | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     -0.0993\n",
      "                          | scaler PowerTransformer() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "SVC_c50                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "SVC_c10                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "SVC_c50                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "SVC_c10                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler PowerTransformer() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "LogisticRegression        | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "LogisticRegression        | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler MinMaxScaler() | rep method PiecewiseAggregateApproximation(n_segments=16)\n",
      "LogisticRegression        | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     -0.0993\n",
      "                          | scaler StandardScaler() | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "SVC_c50                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "SVC_c10                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "SVC_c50                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "SVC_c10                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler StandardScaler() | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "LogisticRegression        | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     -0.0993\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method PiecewiseAggregateApproximation(n_segments=12)\n",
      "SVC_c50                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "SVC_c10                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10)\n",
      "SVC_c50                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n",
      "SVC_c10                   | precision     0.0000 | recall     0.0000 | f1     0.0000| mcc     0.0000\n",
      "                          | scaler TimeSeriesScalerMeanVariance(mu=0, std=1) | rep method SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.09933992677987828),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.6333333333333333,\n",
       "  0.5757575757575758,\n",
       "  0.6031746031746033,\n",
       "  0.3305736828770171),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.5172413793103449,\n",
       "  0.45454545454545453,\n",
       "  0.4838709677419355,\n",
       "  0.1392715036327889),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.6,\n",
       "  0.45454545454545453,\n",
       "  0.5172413793103449,\n",
       "  0.2401922307076307),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.5,\n",
       "  0.3939393939393939,\n",
       "  0.4406779661016949,\n",
       "  0.1030578237334737),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.4642857142857143,\n",
       "  0.3939393939393939,\n",
       "  0.4262295081967213,\n",
       "  0.0545544725589981),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.20454545454545456),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.20454545454545456),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.4878048780487805,\n",
       "  0.6060606060606061,\n",
       "  0.5405405405405406,\n",
       "  0.12773653157788623),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.42857142857142855,\n",
       "  1.0,\n",
       "  0.6,\n",
       "  0.0),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.5483870967741935,\n",
       "  0.5151515151515151,\n",
       "  0.53125,\n",
       "  0.19875724220194652),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.4864864864864865,\n",
       "  0.5454545454545454,\n",
       "  0.5142857142857143,\n",
       "  0.11255629222268704),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.5714285714285714,\n",
       "  0.6060606060606061,\n",
       "  0.588235294117647,\n",
       "  0.2635231383473649),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.5526315789473685,\n",
       "  0.6363636363636364,\n",
       "  0.5915492957746479,\n",
       "  0.24745669886621613),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.25,\n",
       "  0.030303030303030304,\n",
       "  0.054054054054054064,\n",
       "  -0.08446717229993574),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.47058823529411764,\n",
       "  0.48484848484848486,\n",
       "  0.4776119402985075,\n",
       "  0.07549804236114203),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.4166666666666667,\n",
       "  0.45454545454545453,\n",
       "  0.43478260869565216,\n",
       "  -0.022541740866685808),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.5135135135135135,\n",
       "  0.5757575757575758,\n",
       "  0.5428571428571428,\n",
       "  0.16508256192660767),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.5333333333333333,\n",
       "  0.48484848484848486,\n",
       "  0.507936507936508,\n",
       "  0.16913072147196226),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.4827586206896552,\n",
       "  0.42424242424242425,\n",
       "  0.45161290322580644,\n",
       "  0.08511036333114877),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.5,\n",
       "  0.36363636363636365,\n",
       "  0.4210526315789474,\n",
       "  0.09712858623572641),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.5,\n",
       "  0.36363636363636365,\n",
       "  0.4210526315789474,\n",
       "  0.09712858623572641),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.5853658536585366,\n",
       "  0.7272727272727273,\n",
       "  0.6486486486486487,\n",
       "  0.3381261130002871),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.5454545454545454,\n",
       "  0.36363636363636365,\n",
       "  0.43636363636363634,\n",
       "  0.14937887931959076),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.6666666666666666,\n",
       "  0.42424242424242425,\n",
       "  0.5185185185185185,\n",
       "  0.29462782549439476),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.6206896551724138,\n",
       "  0.5454545454545454,\n",
       "  0.5806451612903226,\n",
       "  0.3017549245377093),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.5789473684210527,\n",
       "  0.6666666666666666,\n",
       "  0.619718309859155,\n",
       "  0.29994751377723167),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.5833333333333334,\n",
       "  0.6363636363636364,\n",
       "  0.6086956521739131,\n",
       "  0.2930426312669155),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.5384615384615384,\n",
       "  0.42424242424242425,\n",
       "  0.47457627118644075,\n",
       "  0.158550498051498),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.45454545454545453,\n",
       "  0.45454545454545453,\n",
       "  0.45454545454545453,\n",
       "  0.045454545454545456),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.43243243243243246,\n",
       "  0.48484848484848486,\n",
       "  0.45714285714285713,\n",
       "  0.007503752814845803),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.45454545454545453,\n",
       "  0.45454545454545453,\n",
       "  0.45454545454545453,\n",
       "  0.045454545454545456),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.4838709677419355,\n",
       "  0.45454545454545453,\n",
       "  0.46874999999999994,\n",
       "  0.09173411178551377),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.46153846153846156,\n",
       "  0.36363636363636365,\n",
       "  0.4067796610169492,\n",
       "  0.047565149415449405),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.5714285714285714,\n",
       "  0.48484848484848486,\n",
       "  0.5245901639344263,\n",
       "  0.2182178902359924),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.5714285714285714,\n",
       "  0.48484848484848486,\n",
       "  0.5245901639344263,\n",
       "  0.2182178902359924),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.5151515151515151,\n",
       "  0.5151515151515151,\n",
       "  0.5151515151515151,\n",
       "  0.15151515151515152),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.42857142857142855,\n",
       "  1.0,\n",
       "  0.6,\n",
       "  0.0),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.5652173913043478,\n",
       "  0.3939393939393939,\n",
       "  0.4642857142857143,\n",
       "  0.18020683530042983),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5151515151515151,\n",
       "  0.5151515151515151,\n",
       "  0.5151515151515151,\n",
       "  0.15151515151515152),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.5151515151515151,\n",
       "  0.5151515151515151,\n",
       "  0.5151515151515151,\n",
       "  0.15151515151515152),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.5806451612903226,\n",
       "  0.5454545454545454,\n",
       "  0.5625,\n",
       "  0.2522688074101629),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.32,\n",
       "  0.24242424242424243,\n",
       "  0.2758620689655172,\n",
       "  -0.15212174611483278),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.5384615384615384,\n",
       "  0.42424242424242425,\n",
       "  0.47457627118644075,\n",
       "  0.158550498051498),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.20454545454545456),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.45454545454545453,\n",
       "  0.45454545454545453,\n",
       "  0.45454545454545453,\n",
       "  0.045454545454545456),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.6296296296296297,\n",
       "  0.5151515151515151,\n",
       "  0.5666666666666667,\n",
       "  0.29855619650098675),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.6296296296296297,\n",
       "  0.5151515151515151,\n",
       "  0.5666666666666667,\n",
       "  0.29855619650098675),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.5862068965517241,\n",
       "  0.5151515151515151,\n",
       "  0.5483870967741935,\n",
       "  0.24759378423606915),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.4189189189189189,\n",
       "  0.9393939393939394,\n",
       "  0.5794392523364487,\n",
       "  -0.09687303228651607),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.5714285714285714,\n",
       "  0.48484848484848486,\n",
       "  0.5245901639344263,\n",
       "  0.2182178902359924),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.4857142857142857,\n",
       "  0.5151515151515151,\n",
       "  0.5,\n",
       "  0.10540925533894598),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('PowerTransformer',\n",
       "  PowerTransformer(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.5333333333333333,\n",
       "  0.48484848484848486,\n",
       "  0.507936507936508,\n",
       "  0.16913072147196226),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.52,\n",
       "  0.3939393939393939,\n",
       "  0.44827586206896547,\n",
       "  0.12810252304406972),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.5517241379310345,\n",
       "  0.48484848484848486,\n",
       "  0.5161290322580646,\n",
       "  0.19343264393442902),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.4857142857142857,\n",
       "  0.5151515151515151,\n",
       "  0.5,\n",
       "  0.10540925533894598),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.43333333333333335,\n",
       "  0.3939393939393939,\n",
       "  0.4126984126984127,\n",
       "  0.007687760066907375),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.5806451612903226,\n",
       "  0.5454545454545454,\n",
       "  0.5625,\n",
       "  0.2522688074101629),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.5806451612903226,\n",
       "  0.5454545454545454,\n",
       "  0.5625,\n",
       "  0.2522688074101629),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.5833333333333334,\n",
       "  0.6363636363636364,\n",
       "  0.6086956521739131,\n",
       "  0.2930426312669155),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.42857142857142855,\n",
       "  1.0,\n",
       "  0.6,\n",
       "  0.0),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.6071428571428571,\n",
       "  0.5151515151515151,\n",
       "  0.5573770491803278,\n",
       "  0.2727723627949905),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.6333333333333333,\n",
       "  0.5757575757575758,\n",
       "  0.6031746031746033,\n",
       "  0.3305736828770171),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.5769230769230769,\n",
       "  0.45454545454545453,\n",
       "  0.5084745762711863,\n",
       "  0.21404317236952233),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.36363636363636365,\n",
       "  0.12121212121212122,\n",
       "  0.18181818181818182,\n",
       "  -0.05356869554443541),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.55,\n",
       "  0.6666666666666666,\n",
       "  0.6027397260273972,\n",
       "  0.2551275957047573),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.4444444444444444,\n",
       "  0.48484848484848486,\n",
       "  0.463768115942029,\n",
       "  0.030055654488914407),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.5384615384615384,\n",
       "  0.6363636363636364,\n",
       "  0.5833333333333334,\n",
       "  0.22496063533292376),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.5853658536585366,\n",
       "  0.7272727272727273,\n",
       "  0.6486486486486487,\n",
       "  0.3381261130002871),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.4482758620689655,\n",
       "  0.3939393939393939,\n",
       "  0.41935483870967744,\n",
       "  0.030949223029508643),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.5416666666666666,\n",
       "  0.3939393939393939,\n",
       "  0.45614035087719296,\n",
       "  0.15378692820656684),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.5416666666666666,\n",
       "  0.3939393939393939,\n",
       "  0.45614035087719296,\n",
       "  0.15378692820656684),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.5641025641025641,\n",
       "  0.6666666666666666,\n",
       "  0.611111111111111,\n",
       "  0.2774514502439393),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.42857142857142855,\n",
       "  1.0,\n",
       "  0.6,\n",
       "  0.0),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.6071428571428571,\n",
       "  0.5151515151515151,\n",
       "  0.5573770491803278,\n",
       "  0.2727723627949905),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5675675675675675,\n",
       "  0.6363636363636364,\n",
       "  0.6000000000000001,\n",
       "  0.2701351013344489),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.5172413793103449,\n",
       "  0.45454545454545453,\n",
       "  0.4838709677419355,\n",
       "  0.1392715036327889),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.7142857142857143,\n",
       "  0.15151515151515152,\n",
       "  0.25,\n",
       "  0.18257418583505536),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.3076923076923077,\n",
       "  0.12121212121212122,\n",
       "  0.17391304347826086,\n",
       "  -0.1100881057409974),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.45161290322580644,\n",
       "  0.42424242424242425,\n",
       "  0.4375,\n",
       "  0.03822254657729741),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.3939393939393939,\n",
       "  0.3939393939393939,\n",
       "  0.3939393939393939,\n",
       "  -0.06060606060606061),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.47058823529411764,\n",
       "  0.48484848484848486,\n",
       "  0.4776119402985075,\n",
       "  0.07549804236114203),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.48148148148148145,\n",
       "  0.3939393939393939,\n",
       "  0.43333333333333324,\n",
       "  0.07856742013183862),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.5172413793103449,\n",
       "  0.45454545454545453,\n",
       "  0.4838709677419355,\n",
       "  0.1392715036327889),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.5555555555555556,\n",
       "  0.6060606060606061,\n",
       "  0.5797101449275361,\n",
       "  0.24044523591131525),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.5555555555555556,\n",
       "  0.6060606060606061,\n",
       "  0.5797101449275361,\n",
       "  0.24044523591131525),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.5945945945945946,\n",
       "  0.6666666666666666,\n",
       "  0.6285714285714286,\n",
       "  0.3226613710383695),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.42857142857142855,\n",
       "  1.0,\n",
       "  0.6,\n",
       "  0.0),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.5769230769230769,\n",
       "  0.45454545454545453,\n",
       "  0.5084745762711863,\n",
       "  0.21404317236952233),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5625,\n",
       "  0.5454545454545454,\n",
       "  0.5538461538461538,\n",
       "  0.22821773229381923),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.6666666666666666,\n",
       "  0.36363636363636365,\n",
       "  0.4705882352941177,\n",
       "  0.26574700172636695),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.6666666666666666,\n",
       "  0.36363636363636365,\n",
       "  0.4705882352941177,\n",
       "  0.26574700172636695),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.5384615384615384,\n",
       "  0.42424242424242425,\n",
       "  0.47457627118644075,\n",
       "  0.158550498051498),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.52,\n",
       "  0.3939393939393939,\n",
       "  0.44827586206896547,\n",
       "  0.12810252304406972),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.5,\n",
       "  0.48484848484848486,\n",
       "  0.49230769230769234,\n",
       "  0.12171612389003693),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.5384615384615384,\n",
       "  0.42424242424242425,\n",
       "  0.47457627118644075,\n",
       "  0.158550498051498),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.3870967741935484,\n",
       "  0.36363636363636365,\n",
       "  0.37500000000000006,\n",
       "  -0.06880058383913533),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.4074074074074074,\n",
       "  0.3333333333333333,\n",
       "  0.36666666666666664,\n",
       "  -0.03142696805273545),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.56,\n",
       "  0.42424242424242425,\n",
       "  0.4827586206896552,\n",
       "  0.18414737687585023),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.56,\n",
       "  0.42424242424242425,\n",
       "  0.4827586206896552,\n",
       "  0.18414737687585023),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.7142857142857143,\n",
       "  0.45454545454545453,\n",
       "  0.5555555555555556,\n",
       "  0.35355339059327373),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.42857142857142855,\n",
       "  1.0,\n",
       "  0.6,\n",
       "  0.0),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.5384615384615384,\n",
       "  0.42424242424242425,\n",
       "  0.47457627118644075,\n",
       "  0.158550498051498),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5,\n",
       "  0.42424242424242425,\n",
       "  0.4590163934426229,\n",
       "  0.1091089451179962),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.25,\n",
       "  0.030303030303030304,\n",
       "  0.054054054054054064,\n",
       "  -0.08446717229993574),\n",
       " ('MinMaxScaler',\n",
       "  MinMaxScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.25,\n",
       "  0.030303030303030304,\n",
       "  0.054054054054054064,\n",
       "  -0.08446717229993574),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.09933992677987828),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.6285714285714286,\n",
       "  0.6666666666666666,\n",
       "  0.6470588235294118,\n",
       "  0.36893239368631087),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.20454545454545456),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.53125,\n",
       "  0.5151515151515151,\n",
       "  0.5230769230769231,\n",
       "  0.17496692809192807),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.5333333333333333,\n",
       "  0.48484848484848486,\n",
       "  0.507936507936508,\n",
       "  0.16913072147196226),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.4642857142857143,\n",
       "  0.3939393939393939,\n",
       "  0.4262295081967213,\n",
       "  0.0545544725589981),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.53125,\n",
       "  0.5151515151515151,\n",
       "  0.5230769230769231,\n",
       "  0.17496692809192807),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.53125,\n",
       "  0.5151515151515151,\n",
       "  0.5230769230769231,\n",
       "  0.17496692809192807),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.47619047619047616,\n",
       "  0.6060606060606061,\n",
       "  0.5333333333333333,\n",
       "  0.10540925533894598),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.42857142857142855,\n",
       "  1.0,\n",
       "  0.6,\n",
       "  0.0),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.20454545454545456),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5714285714285714,\n",
       "  0.6060606060606061,\n",
       "  0.588235294117647,\n",
       "  0.2635231383473649),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.5675675675675675,\n",
       "  0.6363636363636364,\n",
       "  0.6000000000000001,\n",
       "  0.2701351013344489),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.5405405405405406,\n",
       "  0.6060606060606061,\n",
       "  0.5714285714285714,\n",
       "  0.2176088316305283),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.25,\n",
       "  0.030303030303030304,\n",
       "  0.054054054054054064,\n",
       "  -0.08446717229993574),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.5405405405405406,\n",
       "  0.6060606060606061,\n",
       "  0.5714285714285714,\n",
       "  0.2176088316305283),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.42857142857142855,\n",
       "  0.45454545454545453,\n",
       "  0.4411764705882353,\n",
       "  0.0),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.5,\n",
       "  0.5454545454545454,\n",
       "  0.5217391304347826,\n",
       "  0.13525044520011484),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.6086956521739131,\n",
       "  0.42424242424242425,\n",
       "  0.5,\n",
       "  0.23754537380511206),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.4482758620689655,\n",
       "  0.3939393939393939,\n",
       "  0.41935483870967744,\n",
       "  0.030949223029508643),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.4583333333333333,\n",
       "  0.3333333333333333,\n",
       "  0.38596491228070173,\n",
       "  0.040470244264886004),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.4583333333333333,\n",
       "  0.3333333333333333,\n",
       "  0.38596491228070173,\n",
       "  0.040470244264886004),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.627906976744186,\n",
       "  0.8181818181818182,\n",
       "  0.7105263157894738,\n",
       "  0.4529882541668522),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.5454545454545454,\n",
       "  0.36363636363636365,\n",
       "  0.43636363636363634,\n",
       "  0.14937887931959076),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.6,\n",
       "  0.36363636363636365,\n",
       "  0.4528301886792453,\n",
       "  0.20519567041703082),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5625,\n",
       "  0.5454545454545454,\n",
       "  0.5538461538461538,\n",
       "  0.22821773229381923),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.5853658536585366,\n",
       "  0.7272727272727273,\n",
       "  0.6486486486486487,\n",
       "  0.3381261130002871),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.5789473684210527,\n",
       "  0.6666666666666666,\n",
       "  0.619718309859155,\n",
       "  0.29994751377723167),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.5416666666666666,\n",
       "  0.3939393939393939,\n",
       "  0.45614035087719296,\n",
       "  0.15378692820656684),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.5,\n",
       "  0.48484848484848486,\n",
       "  0.49230769230769234,\n",
       "  0.12171612389003693),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.4,\n",
       "  0.42424242424242425,\n",
       "  0.411764705882353,\n",
       "  -0.05270462766947299),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.45454545454545453,\n",
       "  0.45454545454545453,\n",
       "  0.45454545454545453,\n",
       "  0.045454545454545456),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.3333333333333333,\n",
       "  0.2727272727272727,\n",
       "  0.3,\n",
       "  -0.1414213562373095),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.5,\n",
       "  0.3939393939393939,\n",
       "  0.4406779661016949,\n",
       "  0.1030578237334737),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.6129032258064516,\n",
       "  0.5757575757575758,\n",
       "  0.59375,\n",
       "  0.30578037261837926),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.6129032258064516,\n",
       "  0.5757575757575758,\n",
       "  0.59375,\n",
       "  0.30578037261837926),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.5142857142857142,\n",
       "  0.5454545454545454,\n",
       "  0.5294117647058822,\n",
       "  0.15811388300841894),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.42857142857142855,\n",
       "  1.0,\n",
       "  0.6,\n",
       "  0.0),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.5454545454545454,\n",
       "  0.36363636363636365,\n",
       "  0.43636363636363634,\n",
       "  0.14937887931959076),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5588235294117647,\n",
       "  0.5757575757575758,\n",
       "  0.5671641791044776,\n",
       "  0.2340439313195403),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.5454545454545454,\n",
       "  0.20454545454545456),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.5625,\n",
       "  0.5454545454545454,\n",
       "  0.5538461538461538,\n",
       "  0.22821773229381923),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.39285714285714285,\n",
       "  0.3333333333333333,\n",
       "  0.360655737704918,\n",
       "  -0.0545544725589981),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.5483870967741935,\n",
       "  0.5151515151515151,\n",
       "  0.53125,\n",
       "  0.19875724220194652),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.5,\n",
       "  0.42424242424242425,\n",
       "  0.4590163934426229,\n",
       "  0.1091089451179962),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.4375,\n",
       "  0.42424242424242425,\n",
       "  0.43076923076923074,\n",
       "  0.015214515486254616),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.6296296296296297,\n",
       "  0.5151515151515151,\n",
       "  0.5666666666666667,\n",
       "  0.29855619650098675),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.6296296296296297,\n",
       "  0.5151515151515151,\n",
       "  0.5666666666666667,\n",
       "  0.29855619650098675),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.6060606060606061,\n",
       "  0.6060606060606061,\n",
       "  0.6060606060606061,\n",
       "  0.3106060606060606),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.4189189189189189,\n",
       "  0.9393939393939394,\n",
       "  0.5794392523364487,\n",
       "  -0.09687303228651607),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.6666666666666666,\n",
       "  0.48484848484848486,\n",
       "  0.5614035087719298,\n",
       "  0.32376195411908804),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5142857142857142,\n",
       "  0.5454545454545454,\n",
       "  0.5294117647058822,\n",
       "  0.15811388300841894),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('StandardScaler',\n",
       "  StandardScaler(),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.09933992677987828),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.5294117647058824,\n",
       "  0.5454545454545454,\n",
       "  0.5373134328358209,\n",
       "  0.1811953016667409),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.5581395348837209,\n",
       "  0.7272727272727273,\n",
       "  0.6315789473684211,\n",
       "  0.29444236520845396),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.5405405405405406,\n",
       "  0.6060606060606061,\n",
       "  0.5714285714285714,\n",
       "  0.2176088316305283),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.6060606060606061,\n",
       "  0.6060606060606061,\n",
       "  0.6060606060606061,\n",
       "  0.3106060606060606),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.4642857142857143,\n",
       "  0.3939393939393939,\n",
       "  0.4262295081967213,\n",
       "  0.0545544725589981),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.59375,\n",
       "  0.5757575757575758,\n",
       "  0.5846153846153846,\n",
       "  0.2814685364957104),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.59375,\n",
       "  0.5757575757575758,\n",
       "  0.5846153846153846,\n",
       "  0.2814685364957104),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.4634146341463415,\n",
       "  0.5757575757575758,\n",
       "  0.5135135135135135,\n",
       "  0.07513913622228602),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.42857142857142855,\n",
       "  1.0,\n",
       "  0.6,\n",
       "  0.0),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.5925925925925926,\n",
       "  0.48484848484848486,\n",
       "  0.5333333333333333,\n",
       "  0.2435590024086997),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5862068965517241,\n",
       "  0.5151515151515151,\n",
       "  0.5483870967741935,\n",
       "  0.24759378423606915),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.5384615384615384,\n",
       "  0.6363636363636364,\n",
       "  0.5833333333333334,\n",
       "  0.22496063533292376),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns10',\n",
       "  PiecewiseAggregateApproximation(n_segments=12),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.5405405405405406,\n",
       "  0.6060606060606061,\n",
       "  0.5714285714285714,\n",
       "  0.2176088316305283),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.25,\n",
       "  0.030303030303030304,\n",
       "  0.054054054054054064,\n",
       "  -0.08446717229993574),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.56,\n",
       "  0.42424242424242425,\n",
       "  0.4827586206896552,\n",
       "  0.18414737687585023),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.5,\n",
       "  0.42424242424242425,\n",
       "  0.4590163934426229,\n",
       "  0.1091089451179962),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.6,\n",
       "  0.36363636363636365,\n",
       "  0.4528301886792453,\n",
       "  0.20519567041703082),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.4782608695652174,\n",
       "  0.3333333333333333,\n",
       "  0.3928571428571428,\n",
       "  0.06552975829106539),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.4482758620689655,\n",
       "  0.3939393939393939,\n",
       "  0.41935483870967744,\n",
       "  0.030949223029508643),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.4782608695652174,\n",
       "  0.3333333333333333,\n",
       "  0.3928571428571428,\n",
       "  0.06552975829106539),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.4782608695652174,\n",
       "  0.3333333333333333,\n",
       "  0.3928571428571428,\n",
       "  0.06552975829106539),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.5641025641025641,\n",
       "  0.6666666666666666,\n",
       "  0.611111111111111,\n",
       "  0.2774514502439393),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.5416666666666666,\n",
       "  0.3939393939393939,\n",
       "  0.45614035087719296,\n",
       "  0.15378692820656684),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.52,\n",
       "  0.3939393939393939,\n",
       "  0.44827586206896547,\n",
       "  0.12810252304406972),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5517241379310345,\n",
       "  0.48484848484848486,\n",
       "  0.5161290322580646,\n",
       "  0.19343264393442902),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.575,\n",
       "  0.696969696969697,\n",
       "  0.6301369863013698,\n",
       "  0.3076538654086779),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'PiecewiseAggregateApproximation_ns16',\n",
       "  PiecewiseAggregateApproximation(n_segments=16),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.5555555555555556,\n",
       "  0.6060606060606061,\n",
       "  0.5797101449275361,\n",
       "  0.24044523591131525),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.5,\n",
       "  0.42424242424242425,\n",
       "  0.4590163934426229,\n",
       "  0.1091089451179962),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.5161290322580645,\n",
       "  0.48484848484848486,\n",
       "  0.5,\n",
       "  0.14524567699373014),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.3783783783783784,\n",
       "  0.42424242424242425,\n",
       "  0.4000000000000001,\n",
       "  -0.09754878659299544),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.4838709677419355,\n",
       "  0.45454545454545453,\n",
       "  0.46874999999999994,\n",
       "  0.09173411178551377),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.5588235294117647,\n",
       "  0.5757575757575758,\n",
       "  0.5671641791044776,\n",
       "  0.2340439313195403),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.48,\n",
       "  0.36363636363636365,\n",
       "  0.41379310344827586,\n",
       "  0.07205766921228922),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.59375,\n",
       "  0.5757575757575758,\n",
       "  0.5846153846153846,\n",
       "  0.2814685364957104),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.6060606060606061,\n",
       "  0.6060606060606061,\n",
       "  0.6060606060606061,\n",
       "  0.3106060606060606),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.5555555555555556,\n",
       "  0.6060606060606061,\n",
       "  0.5797101449275361,\n",
       "  0.24044523591131525),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.42857142857142855,\n",
       "  1.0,\n",
       "  0.6,\n",
       "  0.0),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.5925925925925926,\n",
       "  0.48484848484848486,\n",
       "  0.5333333333333333,\n",
       "  0.2435590024086997),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5666666666666667,\n",
       "  0.5151515151515151,\n",
       "  0.5396825396825397,\n",
       "  0.22294504194031387),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=10),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'LogisticRegression',\n",
       "  LogisticRegression(C=0.01),\n",
       "  0.4857142857142857,\n",
       "  0.5151515151515151,\n",
       "  0.5,\n",
       "  0.10540925533894598),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_maxd10',\n",
       "  DecisionTreeClassifier(max_depth=10),\n",
       "  0.53125,\n",
       "  0.5151515151515151,\n",
       "  0.5230769230769231,\n",
       "  0.17496692809192807),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_minsl20',\n",
       "  DecisionTreeClassifier(min_samples_leaf=5),\n",
       "  0.43333333333333335,\n",
       "  0.3939393939393939,\n",
       "  0.4126984126984127,\n",
       "  0.007687760066907375),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_critgini',\n",
       "  DecisionTreeClassifier(),\n",
       "  0.5405405405405406,\n",
       "  0.6060606060606061,\n",
       "  0.5714285714285714,\n",
       "  0.2176088316305283),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'DecisionTree_critentropy',\n",
       "  DecisionTreeClassifier(criterion='entropy'),\n",
       "  0.6071428571428571,\n",
       "  0.5151515151515151,\n",
       "  0.5573770491803278,\n",
       "  0.2727723627949905),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'GaussianNB',\n",
       "  GaussianNB(),\n",
       "  0.47058823529411764,\n",
       "  0.48484848484848486,\n",
       "  0.4776119402985075,\n",
       "  0.07549804236114203),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNN_K1_wdist',\n",
       "  KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
       "  0.5925925925925926,\n",
       "  0.48484848484848486,\n",
       "  0.5333333333333333,\n",
       "  0.2435590024086997),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_eu',\n",
       "  KNeighborsTimeSeriesClassifier(metric='euclidean', n_neighbors=1),\n",
       "  0.5925925925925926,\n",
       "  0.48484848484848486,\n",
       "  0.5333333333333333,\n",
       "  0.2435590024086997),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_dtw',\n",
       "  KNeighborsTimeSeriesClassifier(n_neighbors=1),\n",
       "  0.6071428571428571,\n",
       "  0.5151515151515151,\n",
       "  0.5573770491803278,\n",
       "  0.2727723627949905),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'KNNTM_K1_sax',\n",
       "  KNeighborsTimeSeriesClassifier(metric='sax',\n",
       "                                 metric_params={'alphabet_size_avg': 10,\n",
       "                                                'n_segments': 16},\n",
       "                                 n_neighbors=1),\n",
       "  0.41333333333333333,\n",
       "  0.9393939393939394,\n",
       "  0.5740740740740741,\n",
       "  -0.18856180831641267),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'RandomForestClassifier_ne50',\n",
       "  RandomForestClassifier(n_estimators=50, random_state=0),\n",
       "  0.5769230769230769,\n",
       "  0.45454545454545453,\n",
       "  0.5084745762711863,\n",
       "  0.21404317236952233),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'RandomForestClassifier_ne10',\n",
       "  RandomForestClassifier(n_estimators=10, random_state=0),\n",
       "  0.5,\n",
       "  0.5454545454545454,\n",
       "  0.5217391304347826,\n",
       "  0.13525044520011484),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'SVC_c50',\n",
       "  SVC(C=50, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " ('TimeSeriesScalerMeanVariance',\n",
       "  TimeSeriesScalerMeanVariance(mu=0, std=1),\n",
       "  'SymbolicAggregateApproximation_ns10',\n",
       "  SymbolicAggregateApproximation(alphabet_size_avg=40, n_segments=32),\n",
       "  'SVC_c10',\n",
       "  SVC(C=10, gamma='auto'),\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a list of scalers\n",
    "scalers = [\n",
    "    ('PowerTransformer', PowerTransformer()),\n",
    "    ('MinMaxScaler', MinMaxScaler()),\n",
    "    ('StandardScaler', StandardScaler()),\n",
    "    ('TimeSeriesScalerMeanVariance', TimeSeriesScalerMeanVariance(mu=0, std=1))\n",
    "]\n",
    "\n",
    "# Defining a list of representation methods\n",
    "representation_models = [\n",
    "    ('PiecewiseAggregateApproximation_ns10', PiecewiseAggregateApproximation(n_segments=12)),\n",
    "    ('PiecewiseAggregateApproximation_ns16', PiecewiseAggregateApproximation(n_segments=16)),\n",
    "    ('SymbolicAggregateApproximation_ns10', SymbolicAggregateApproximation(n_segments=10, alphabet_size_avg=40)),\n",
    "    ('SymbolicAggregateApproximation_ns10', SymbolicAggregateApproximation(n_segments=32, alphabet_size_avg=40))\n",
    "]\n",
    "\n",
    "dict = {'n_segments' : 16 , 'alphabet_size_avg': 10}\n",
    "\n",
    "# Defining a list of classification models\n",
    "classification_models = [\n",
    "    ('LogisticRegression', LogisticRegression(C = 0.01)),\n",
    "    ('DecisionTree_maxd10', DecisionTreeClassifier(max_depth = 10)),\n",
    "    ('DecisionTree_minsl20', DecisionTreeClassifier(min_samples_leaf = 5)),\n",
    "    ('DecisionTree_critgini', DecisionTreeClassifier(criterion = 'gini')),\n",
    "    ('DecisionTree_critentropy', DecisionTreeClassifier(criterion = 'entropy')),\n",
    "    ('GaussianNB', GaussianNB()),\n",
    "    ('KNN_K1_wdist', KNeighborsClassifier(n_neighbors = 1,weights = 'distance')),\n",
    "    ('KNNTM_K1_eu', KNeighborsTimeSeriesClassifier(n_neighbors = 1, metric = 'euclidean')),\n",
    "    ('KNNTM_K1_dtw', KNeighborsTimeSeriesClassifier(n_neighbors = 1, metric = 'dtw')),\n",
    "    ('KNNTM_K1_sax', KNeighborsTimeSeriesClassifier(n_neighbors = 1, metric = 'sax',metric_params = dict)),\n",
    "    ('RandomForestClassifier_ne50',RandomForestClassifier(n_estimators=50, random_state=0)),\n",
    "    ('RandomForestClassifier_ne10',RandomForestClassifier(n_estimators=10, random_state=0)),\n",
    "    ('SVC_c50',SVC(C=50,gamma='auto')),\n",
    "    ('SVC_c10',SVC(C=10,gamma='auto'))\n",
    "]\n",
    "\n",
    "SHOW_REP_MODEL = False #false will not present the representation methods\n",
    "SHOW_MODEL = False #True to present/print the progress of the model performance\n",
    "\n",
    "#First run will be just with the scalers and the classification models\n",
    "test_models(scalers, representation_models, classification_models, X_train, y_train, X_test,y_test, SHOW_REP_MODEL, SHOW_MODEL)\n",
    "\n",
    "SHOW_REP_MODEL = True\n",
    "\n",
    "#Second run, for representation models\n",
    "test_models(scalers, representation_models, classification_models, X_train, y_train, X_test,y_test, SHOW_REP_MODEL, SHOW_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2 of the project is most likely to be done using the info in the TP7 timeseries forecasting i believe \n",
    "\n",
    "#histograms in EDA its  NO GO, too many rows in the datasets\n",
    "#Same for presenting the plot for outliers, FIND ANOTHER WAY, counting them its a solution\n",
    "\n",
    "#DONT KNOW if i have to check the outliers of the test set also \n",
    "#VERIFY if a matrix of correlations is possibleor not, sounds difficult as we dont have a target variable\n",
    "\n",
    "#Scaling data might need to be done VERIFY  the 4 Scalers PowerTransformer, StandardScaler , MinMaxScaler and the last one is the normalizer i believe it might be useful as it works with rows check AA notes/slides, no need for imputation though\n",
    "#in the Project statement when she says find the best classifier model, in the TP6 she only uses the KNeighborsClassifier CHECK if its possible to use other classification models or if its even needed to\n",
    "\n",
    "#SEE IF SCALING IS NEEDED\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
