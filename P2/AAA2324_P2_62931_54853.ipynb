{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning 2nd Project\n",
    "### Authors: Guilherme Cepeda - 62931, Pedro Serrano - 54853\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
    "from tslearn.piecewise import PiecewiseAggregateApproximation, SymbolicAggregateApproximation\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 181 entries, 0 to 180\n",
      "Columns: 901 entries, class to t900\n",
      "dtypes: float64(901)\n",
      "memory usage: 1.2 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77 entries, 0 to 76\n",
      "Columns: 901 entries, class to t900\n",
      "dtypes: float64(901)\n",
      "memory usage: 542.1 KB\n",
      "None\n",
      "(181, 901)\n",
      "(77, 901)\n"
     ]
    }
   ],
   "source": [
    "#creates a dataframe from a file\n",
    "\n",
    "#the csv has no column names, so we have to add them\n",
    "list=['class']\n",
    "for i in range(1,901):\n",
    "    list.append('t' + str(i))\n",
    "\n",
    "#we created a list [class, t0 - t900] to represent the 901 columns we have\n",
    "#names= list makes this list as a header to the dataframe\n",
    "df_trainset = pd.read_csv(\"worms_trainset.csv\", names=list)\n",
    "\n",
    "df_testset = pd.read_csv(\"worms_testset.csv\", names=list)\n",
    "\n",
    "#info\n",
    "print(df_trainset.info())\n",
    "\n",
    "#info\n",
    "print(df_testset.info())\n",
    "\n",
    "print(df_trainset.shape)\n",
    "print(df_testset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 't1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10', 't11', 't12', 't13', 't14', 't15', 't16', 't17', 't18', 't19', 't20', 't21', 't22', 't23', 't24', 't25', 't26', 't27', 't28', 't29', 't30', 't31', 't32', 't33', 't34', 't35', 't36', 't37', 't38', 't39', 't40', 't41', 't42', 't43', 't44', 't45', 't46', 't47', 't48', 't49', 't50', 't51', 't52', 't53', 't54', 't55', 't56', 't57', 't58', 't59', 't60', 't61', 't62', 't63', 't64', 't65', 't66', 't67', 't68', 't69', 't70', 't71', 't72', 't73', 't74', 't75', 't76', 't77', 't78', 't79', 't80', 't81', 't82', 't83', 't84', 't85', 't86', 't87', 't88', 't89', 't90', 't91', 't92', 't93', 't94', 't95', 't96', 't97', 't98', 't99', 't100', 't101', 't102', 't103', 't104', 't105', 't106', 't107', 't108', 't109', 't110', 't111', 't112', 't113', 't114', 't115', 't116', 't117', 't118', 't119', 't120', 't121', 't122', 't123', 't124', 't125', 't126', 't127', 't128', 't129', 't130', 't131', 't132', 't133', 't134', 't135', 't136', 't137', 't138', 't139', 't140', 't141', 't142', 't143', 't144', 't145', 't146', 't147', 't148', 't149', 't150', 't151', 't152', 't153', 't154', 't155', 't156', 't157', 't158', 't159', 't160', 't161', 't162', 't163', 't164', 't165', 't166', 't167', 't168', 't169', 't170', 't171', 't172', 't173', 't174', 't175', 't176', 't177', 't178', 't179', 't180', 't181', 't182', 't183', 't184', 't185', 't186', 't187', 't188', 't189', 't190', 't191', 't192', 't193', 't194', 't195', 't196', 't197', 't198', 't199', 't200', 't201', 't202', 't203', 't204', 't205', 't206', 't207', 't208', 't209', 't210', 't211', 't212', 't213', 't214', 't215', 't216', 't217', 't218', 't219', 't220', 't221', 't222', 't223', 't224', 't225', 't226', 't227', 't228', 't229', 't230', 't231', 't232', 't233', 't234', 't235', 't236', 't237', 't238', 't239', 't240', 't241', 't242', 't243', 't244', 't245', 't246', 't247', 't248', 't249', 't250', 't251', 't252', 't253', 't254', 't255', 't256', 't257', 't258', 't259', 't260', 't261', 't262', 't263', 't264', 't265', 't266', 't267', 't268', 't269', 't270', 't271', 't272', 't273', 't274', 't275', 't276', 't277', 't278', 't279', 't280', 't281', 't282', 't283', 't284', 't285', 't286', 't287', 't288', 't289', 't290', 't291', 't292', 't293', 't294', 't295', 't296', 't297', 't298', 't299', 't300', 't301', 't302', 't303', 't304', 't305', 't306', 't307', 't308', 't309', 't310', 't311', 't312', 't313', 't314', 't315', 't316', 't317', 't318', 't319', 't320', 't321', 't322', 't323', 't324', 't325', 't326', 't327', 't328', 't329', 't330', 't331', 't332', 't333', 't334', 't335', 't336', 't337', 't338', 't339', 't340', 't341', 't342', 't343', 't344', 't345', 't346', 't347', 't348', 't349', 't350', 't351', 't352', 't353', 't354', 't355', 't356', 't357', 't358', 't359', 't360', 't361', 't362', 't363', 't364', 't365', 't366', 't367', 't368', 't369', 't370', 't371', 't372', 't373', 't374', 't375', 't376', 't377', 't378', 't379', 't380', 't381', 't382', 't383', 't384', 't385', 't386', 't387', 't388', 't389', 't390', 't391', 't392', 't393', 't394', 't395', 't396', 't397', 't398', 't399', 't400', 't401', 't402', 't403', 't404', 't405', 't406', 't407', 't408', 't409', 't410', 't411', 't412', 't413', 't414', 't415', 't416', 't417', 't418', 't419', 't420', 't421', 't422', 't423', 't424', 't425', 't426', 't427', 't428', 't429', 't430', 't431', 't432', 't433', 't434', 't435', 't436', 't437', 't438', 't439', 't440', 't441', 't442', 't443', 't444', 't445', 't446', 't447', 't448', 't449', 't450', 't451', 't452', 't453', 't454', 't455', 't456', 't457', 't458', 't459', 't460', 't461', 't462', 't463', 't464', 't465', 't466', 't467', 't468', 't469', 't470', 't471', 't472', 't473', 't474', 't475', 't476', 't477', 't478', 't479', 't480', 't481', 't482', 't483', 't484', 't485', 't486', 't487', 't488', 't489', 't490', 't491', 't492', 't493', 't494', 't495', 't496', 't497', 't498', 't499', 't500', 't501', 't502', 't503', 't504', 't505', 't506', 't507', 't508', 't509', 't510', 't511', 't512', 't513', 't514', 't515', 't516', 't517', 't518', 't519', 't520', 't521', 't522', 't523', 't524', 't525', 't526', 't527', 't528', 't529', 't530', 't531', 't532', 't533', 't534', 't535', 't536', 't537', 't538', 't539', 't540', 't541', 't542', 't543', 't544', 't545', 't546', 't547', 't548', 't549', 't550', 't551', 't552', 't553', 't554', 't555', 't556', 't557', 't558', 't559', 't560', 't561', 't562', 't563', 't564', 't565', 't566', 't567', 't568', 't569', 't570', 't571', 't572', 't573', 't574', 't575', 't576', 't577', 't578', 't579', 't580', 't581', 't582', 't583', 't584', 't585', 't586', 't587', 't588', 't589', 't590', 't591', 't592', 't593', 't594', 't595', 't596', 't597', 't598', 't599', 't600', 't601', 't602', 't603', 't604', 't605', 't606', 't607', 't608', 't609', 't610', 't611', 't612', 't613', 't614', 't615', 't616', 't617', 't618', 't619', 't620', 't621', 't622', 't623', 't624', 't625', 't626', 't627', 't628', 't629', 't630', 't631', 't632', 't633', 't634', 't635', 't636', 't637', 't638', 't639', 't640', 't641', 't642', 't643', 't644', 't645', 't646', 't647', 't648', 't649', 't650', 't651', 't652', 't653', 't654', 't655', 't656', 't657', 't658', 't659', 't660', 't661', 't662', 't663', 't664', 't665', 't666', 't667', 't668', 't669', 't670', 't671', 't672', 't673', 't674', 't675', 't676', 't677', 't678', 't679', 't680', 't681', 't682', 't683', 't684', 't685', 't686', 't687', 't688', 't689', 't690', 't691', 't692', 't693', 't694', 't695', 't696', 't697', 't698', 't699', 't700', 't701', 't702', 't703', 't704', 't705', 't706', 't707', 't708', 't709', 't710', 't711', 't712', 't713', 't714', 't715', 't716', 't717', 't718', 't719', 't720', 't721', 't722', 't723', 't724', 't725', 't726', 't727', 't728', 't729', 't730', 't731', 't732', 't733', 't734', 't735', 't736', 't737', 't738', 't739', 't740', 't741', 't742', 't743', 't744', 't745', 't746', 't747', 't748', 't749', 't750', 't751', 't752', 't753', 't754', 't755', 't756', 't757', 't758', 't759', 't760', 't761', 't762', 't763', 't764', 't765', 't766', 't767', 't768', 't769', 't770', 't771', 't772', 't773', 't774', 't775', 't776', 't777', 't778', 't779', 't780', 't781', 't782', 't783', 't784', 't785', 't786', 't787', 't788', 't789', 't790', 't791', 't792', 't793', 't794', 't795', 't796', 't797', 't798', 't799', 't800', 't801', 't802', 't803', 't804', 't805', 't806', 't807', 't808', 't809', 't810', 't811', 't812', 't813', 't814', 't815', 't816', 't817', 't818', 't819', 't820', 't821', 't822', 't823', 't824', 't825', 't826', 't827', 't828', 't829', 't830', 't831', 't832', 't833', 't834', 't835', 't836', 't837', 't838', 't839', 't840', 't841', 't842', 't843', 't844', 't845', 't846', 't847', 't848', 't849', 't850', 't851', 't852', 't853', 't854', 't855', 't856', 't857', 't858', 't859', 't860', 't861', 't862', 't863', 't864', 't865', 't866', 't867', 't868', 't869', 't870', 't871', 't872', 't873', 't874', 't875', 't876', 't877', 't878', 't879', 't880', 't881', 't882', 't883', 't884', 't885', 't886', 't887', 't888', 't889', 't890', 't891', 't892', 't893', 't894', 't895', 't896', 't897', 't898', 't899', 't900']\n",
      "\n",
      "\n",
      "['class', 't1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10', 't11', 't12', 't13', 't14', 't15', 't16', 't17', 't18', 't19', 't20', 't21', 't22', 't23', 't24', 't25', 't26', 't27', 't28', 't29', 't30', 't31', 't32', 't33', 't34', 't35', 't36', 't37', 't38', 't39', 't40', 't41', 't42', 't43', 't44', 't45', 't46', 't47', 't48', 't49', 't50', 't51', 't52', 't53', 't54', 't55', 't56', 't57', 't58', 't59', 't60', 't61', 't62', 't63', 't64', 't65', 't66', 't67', 't68', 't69', 't70', 't71', 't72', 't73', 't74', 't75', 't76', 't77', 't78', 't79', 't80', 't81', 't82', 't83', 't84', 't85', 't86', 't87', 't88', 't89', 't90', 't91', 't92', 't93', 't94', 't95', 't96', 't97', 't98', 't99', 't100', 't101', 't102', 't103', 't104', 't105', 't106', 't107', 't108', 't109', 't110', 't111', 't112', 't113', 't114', 't115', 't116', 't117', 't118', 't119', 't120', 't121', 't122', 't123', 't124', 't125', 't126', 't127', 't128', 't129', 't130', 't131', 't132', 't133', 't134', 't135', 't136', 't137', 't138', 't139', 't140', 't141', 't142', 't143', 't144', 't145', 't146', 't147', 't148', 't149', 't150', 't151', 't152', 't153', 't154', 't155', 't156', 't157', 't158', 't159', 't160', 't161', 't162', 't163', 't164', 't165', 't166', 't167', 't168', 't169', 't170', 't171', 't172', 't173', 't174', 't175', 't176', 't177', 't178', 't179', 't180', 't181', 't182', 't183', 't184', 't185', 't186', 't187', 't188', 't189', 't190', 't191', 't192', 't193', 't194', 't195', 't196', 't197', 't198', 't199', 't200', 't201', 't202', 't203', 't204', 't205', 't206', 't207', 't208', 't209', 't210', 't211', 't212', 't213', 't214', 't215', 't216', 't217', 't218', 't219', 't220', 't221', 't222', 't223', 't224', 't225', 't226', 't227', 't228', 't229', 't230', 't231', 't232', 't233', 't234', 't235', 't236', 't237', 't238', 't239', 't240', 't241', 't242', 't243', 't244', 't245', 't246', 't247', 't248', 't249', 't250', 't251', 't252', 't253', 't254', 't255', 't256', 't257', 't258', 't259', 't260', 't261', 't262', 't263', 't264', 't265', 't266', 't267', 't268', 't269', 't270', 't271', 't272', 't273', 't274', 't275', 't276', 't277', 't278', 't279', 't280', 't281', 't282', 't283', 't284', 't285', 't286', 't287', 't288', 't289', 't290', 't291', 't292', 't293', 't294', 't295', 't296', 't297', 't298', 't299', 't300', 't301', 't302', 't303', 't304', 't305', 't306', 't307', 't308', 't309', 't310', 't311', 't312', 't313', 't314', 't315', 't316', 't317', 't318', 't319', 't320', 't321', 't322', 't323', 't324', 't325', 't326', 't327', 't328', 't329', 't330', 't331', 't332', 't333', 't334', 't335', 't336', 't337', 't338', 't339', 't340', 't341', 't342', 't343', 't344', 't345', 't346', 't347', 't348', 't349', 't350', 't351', 't352', 't353', 't354', 't355', 't356', 't357', 't358', 't359', 't360', 't361', 't362', 't363', 't364', 't365', 't366', 't367', 't368', 't369', 't370', 't371', 't372', 't373', 't374', 't375', 't376', 't377', 't378', 't379', 't380', 't381', 't382', 't383', 't384', 't385', 't386', 't387', 't388', 't389', 't390', 't391', 't392', 't393', 't394', 't395', 't396', 't397', 't398', 't399', 't400', 't401', 't402', 't403', 't404', 't405', 't406', 't407', 't408', 't409', 't410', 't411', 't412', 't413', 't414', 't415', 't416', 't417', 't418', 't419', 't420', 't421', 't422', 't423', 't424', 't425', 't426', 't427', 't428', 't429', 't430', 't431', 't432', 't433', 't434', 't435', 't436', 't437', 't438', 't439', 't440', 't441', 't442', 't443', 't444', 't445', 't446', 't447', 't448', 't449', 't450', 't451', 't452', 't453', 't454', 't455', 't456', 't457', 't458', 't459', 't460', 't461', 't462', 't463', 't464', 't465', 't466', 't467', 't468', 't469', 't470', 't471', 't472', 't473', 't474', 't475', 't476', 't477', 't478', 't479', 't480', 't481', 't482', 't483', 't484', 't485', 't486', 't487', 't488', 't489', 't490', 't491', 't492', 't493', 't494', 't495', 't496', 't497', 't498', 't499', 't500', 't501', 't502', 't503', 't504', 't505', 't506', 't507', 't508', 't509', 't510', 't511', 't512', 't513', 't514', 't515', 't516', 't517', 't518', 't519', 't520', 't521', 't522', 't523', 't524', 't525', 't526', 't527', 't528', 't529', 't530', 't531', 't532', 't533', 't534', 't535', 't536', 't537', 't538', 't539', 't540', 't541', 't542', 't543', 't544', 't545', 't546', 't547', 't548', 't549', 't550', 't551', 't552', 't553', 't554', 't555', 't556', 't557', 't558', 't559', 't560', 't561', 't562', 't563', 't564', 't565', 't566', 't567', 't568', 't569', 't570', 't571', 't572', 't573', 't574', 't575', 't576', 't577', 't578', 't579', 't580', 't581', 't582', 't583', 't584', 't585', 't586', 't587', 't588', 't589', 't590', 't591', 't592', 't593', 't594', 't595', 't596', 't597', 't598', 't599', 't600', 't601', 't602', 't603', 't604', 't605', 't606', 't607', 't608', 't609', 't610', 't611', 't612', 't613', 't614', 't615', 't616', 't617', 't618', 't619', 't620', 't621', 't622', 't623', 't624', 't625', 't626', 't627', 't628', 't629', 't630', 't631', 't632', 't633', 't634', 't635', 't636', 't637', 't638', 't639', 't640', 't641', 't642', 't643', 't644', 't645', 't646', 't647', 't648', 't649', 't650', 't651', 't652', 't653', 't654', 't655', 't656', 't657', 't658', 't659', 't660', 't661', 't662', 't663', 't664', 't665', 't666', 't667', 't668', 't669', 't670', 't671', 't672', 't673', 't674', 't675', 't676', 't677', 't678', 't679', 't680', 't681', 't682', 't683', 't684', 't685', 't686', 't687', 't688', 't689', 't690', 't691', 't692', 't693', 't694', 't695', 't696', 't697', 't698', 't699', 't700', 't701', 't702', 't703', 't704', 't705', 't706', 't707', 't708', 't709', 't710', 't711', 't712', 't713', 't714', 't715', 't716', 't717', 't718', 't719', 't720', 't721', 't722', 't723', 't724', 't725', 't726', 't727', 't728', 't729', 't730', 't731', 't732', 't733', 't734', 't735', 't736', 't737', 't738', 't739', 't740', 't741', 't742', 't743', 't744', 't745', 't746', 't747', 't748', 't749', 't750', 't751', 't752', 't753', 't754', 't755', 't756', 't757', 't758', 't759', 't760', 't761', 't762', 't763', 't764', 't765', 't766', 't767', 't768', 't769', 't770', 't771', 't772', 't773', 't774', 't775', 't776', 't777', 't778', 't779', 't780', 't781', 't782', 't783', 't784', 't785', 't786', 't787', 't788', 't789', 't790', 't791', 't792', 't793', 't794', 't795', 't796', 't797', 't798', 't799', 't800', 't801', 't802', 't803', 't804', 't805', 't806', 't807', 't808', 't809', 't810', 't811', 't812', 't813', 't814', 't815', 't816', 't817', 't818', 't819', 't820', 't821', 't822', 't823', 't824', 't825', 't826', 't827', 't828', 't829', 't830', 't831', 't832', 't833', 't834', 't835', 't836', 't837', 't838', 't839', 't840', 't841', 't842', 't843', 't844', 't845', 't846', 't847', 't848', 't849', 't850', 't851', 't852', 't853', 't854', 't855', 't856', 't857', 't858', 't859', 't860', 't861', 't862', 't863', 't864', 't865', 't866', 't867', 't868', 't869', 't870', 't871', 't872', 't873', 't874', 't875', 't876', 't877', 't878', 't879', 't880', 't881', 't882', 't883', 't884', 't885', 't886', 't887', 't888', 't889', 't890', 't891', 't892', 't893', 't894', 't895', 't896', 't897', 't898', 't899', 't900']\n"
     ]
    }
   ],
   "source": [
    "#just checking if everything is alright\n",
    "print(df_trainset.columns.tolist())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(df_testset.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>...</th>\n",
       "      <th>t891</th>\n",
       "      <th>t892</th>\n",
       "      <th>t893</th>\n",
       "      <th>t894</th>\n",
       "      <th>t895</th>\n",
       "      <th>t896</th>\n",
       "      <th>t897</th>\n",
       "      <th>t898</th>\n",
       "      <th>t899</th>\n",
       "      <th>t900</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>181.00000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.58011</td>\n",
       "      <td>-0.054762</td>\n",
       "      <td>-0.055049</td>\n",
       "      <td>-0.053320</td>\n",
       "      <td>-0.044472</td>\n",
       "      <td>-0.046638</td>\n",
       "      <td>-0.054311</td>\n",
       "      <td>-0.060297</td>\n",
       "      <td>-0.064308</td>\n",
       "      <td>-0.059014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097132</td>\n",
       "      <td>0.101416</td>\n",
       "      <td>0.102469</td>\n",
       "      <td>0.088442</td>\n",
       "      <td>0.087919</td>\n",
       "      <td>0.075541</td>\n",
       "      <td>0.070701</td>\n",
       "      <td>0.064898</td>\n",
       "      <td>0.064639</td>\n",
       "      <td>0.055676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.49491</td>\n",
       "      <td>1.231458</td>\n",
       "      <td>1.217009</td>\n",
       "      <td>1.212576</td>\n",
       "      <td>1.211890</td>\n",
       "      <td>1.212344</td>\n",
       "      <td>1.211266</td>\n",
       "      <td>1.216288</td>\n",
       "      <td>1.214133</td>\n",
       "      <td>1.209916</td>\n",
       "      <td>...</td>\n",
       "      <td>1.280118</td>\n",
       "      <td>1.284467</td>\n",
       "      <td>1.264096</td>\n",
       "      <td>1.265234</td>\n",
       "      <td>1.264899</td>\n",
       "      <td>1.274256</td>\n",
       "      <td>1.260681</td>\n",
       "      <td>1.257045</td>\n",
       "      <td>1.278832</td>\n",
       "      <td>1.279609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-3.739104</td>\n",
       "      <td>-3.719033</td>\n",
       "      <td>-3.731076</td>\n",
       "      <td>-3.715019</td>\n",
       "      <td>-3.755160</td>\n",
       "      <td>-3.751146</td>\n",
       "      <td>-3.803328</td>\n",
       "      <td>-3.799314</td>\n",
       "      <td>-3.731076</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.485523</td>\n",
       "      <td>-3.623585</td>\n",
       "      <td>-3.027088</td>\n",
       "      <td>-3.278430</td>\n",
       "      <td>-3.003730</td>\n",
       "      <td>-3.278430</td>\n",
       "      <td>-3.002307</td>\n",
       "      <td>-2.970360</td>\n",
       "      <td>-2.957012</td>\n",
       "      <td>-3.071337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.854008</td>\n",
       "      <td>-0.831337</td>\n",
       "      <td>-0.804410</td>\n",
       "      <td>-0.804410</td>\n",
       "      <td>-0.855404</td>\n",
       "      <td>-0.822698</td>\n",
       "      <td>-0.850160</td>\n",
       "      <td>-0.883371</td>\n",
       "      <td>-0.870900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.904257</td>\n",
       "      <td>-0.882800</td>\n",
       "      <td>-0.877030</td>\n",
       "      <td>-0.874598</td>\n",
       "      <td>-0.849661</td>\n",
       "      <td>-0.846069</td>\n",
       "      <td>-0.849122</td>\n",
       "      <td>-0.934296</td>\n",
       "      <td>-0.920635</td>\n",
       "      <td>-0.931001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.034666</td>\n",
       "      <td>-0.055647</td>\n",
       "      <td>-0.033979</td>\n",
       "      <td>-0.012762</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>-0.038728</td>\n",
       "      <td>-0.014604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101768</td>\n",
       "      <td>0.072891</td>\n",
       "      <td>0.021509</td>\n",
       "      <td>0.045957</td>\n",
       "      <td>0.011206</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>-0.004682</td>\n",
       "      <td>0.035938</td>\n",
       "      <td>0.029757</td>\n",
       "      <td>0.029757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.764857</td>\n",
       "      <td>0.701450</td>\n",
       "      <td>0.795012</td>\n",
       "      <td>0.845768</td>\n",
       "      <td>0.863678</td>\n",
       "      <td>0.849089</td>\n",
       "      <td>0.857694</td>\n",
       "      <td>0.859221</td>\n",
       "      <td>0.853187</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130934</td>\n",
       "      <td>1.173048</td>\n",
       "      <td>1.112793</td>\n",
       "      <td>1.082254</td>\n",
       "      <td>1.079085</td>\n",
       "      <td>1.094289</td>\n",
       "      <td>1.049968</td>\n",
       "      <td>1.053081</td>\n",
       "      <td>1.072255</td>\n",
       "      <td>1.081842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.482405</td>\n",
       "      <td>3.295940</td>\n",
       "      <td>3.109476</td>\n",
       "      <td>2.923011</td>\n",
       "      <td>2.736547</td>\n",
       "      <td>2.603414</td>\n",
       "      <td>2.583677</td>\n",
       "      <td>2.563941</td>\n",
       "      <td>2.542011</td>\n",
       "      <td>...</td>\n",
       "      <td>3.799677</td>\n",
       "      <td>3.934791</td>\n",
       "      <td>3.754639</td>\n",
       "      <td>4.024867</td>\n",
       "      <td>3.799677</td>\n",
       "      <td>3.777158</td>\n",
       "      <td>3.777158</td>\n",
       "      <td>3.574488</td>\n",
       "      <td>3.844715</td>\n",
       "      <td>3.574488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 901 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           class          t1          t2          t3          t4          t5  \\\n",
       "count  181.00000  181.000000  181.000000  181.000000  181.000000  181.000000   \n",
       "mean     1.58011   -0.054762   -0.055049   -0.053320   -0.044472   -0.046638   \n",
       "std      0.49491    1.231458    1.217009    1.212576    1.211890    1.212344   \n",
       "min      1.00000   -3.739104   -3.719033   -3.731076   -3.715019   -3.755160   \n",
       "25%      1.00000   -0.854008   -0.831337   -0.804410   -0.804410   -0.855404   \n",
       "50%      2.00000    0.004370   -0.034666   -0.055647   -0.033979   -0.012762   \n",
       "75%      2.00000    0.764857    0.701450    0.795012    0.845768    0.863678   \n",
       "max      2.00000    3.482405    3.295940    3.109476    2.923011    2.736547   \n",
       "\n",
       "               t6          t7          t8          t9  ...        t891  \\\n",
       "count  181.000000  181.000000  181.000000  181.000000  ...  181.000000   \n",
       "mean    -0.054311   -0.060297   -0.064308   -0.059014  ...    0.097132   \n",
       "std      1.211266    1.216288    1.214133    1.209916  ...    1.280118   \n",
       "min     -3.751146   -3.803328   -3.799314   -3.731076  ...   -3.485523   \n",
       "25%     -0.822698   -0.850160   -0.883371   -0.870900  ...   -0.904257   \n",
       "50%      0.003513    0.003733   -0.038728   -0.014604  ...    0.101768   \n",
       "75%      0.849089    0.857694    0.859221    0.853187  ...    1.130934   \n",
       "max      2.603414    2.583677    2.563941    2.542011  ...    3.799677   \n",
       "\n",
       "             t892        t893        t894        t895        t896        t897  \\\n",
       "count  181.000000  181.000000  181.000000  181.000000  181.000000  181.000000   \n",
       "mean     0.101416    0.102469    0.088442    0.087919    0.075541    0.070701   \n",
       "std      1.284467    1.264096    1.265234    1.264899    1.274256    1.260681   \n",
       "min     -3.623585   -3.027088   -3.278430   -3.003730   -3.278430   -3.002307   \n",
       "25%     -0.882800   -0.877030   -0.874598   -0.849661   -0.846069   -0.849122   \n",
       "50%      0.072891    0.021509    0.045957    0.011206    0.016229   -0.004682   \n",
       "75%      1.173048    1.112793    1.082254    1.079085    1.094289    1.049968   \n",
       "max      3.934791    3.754639    4.024867    3.799677    3.777158    3.777158   \n",
       "\n",
       "             t898        t899        t900  \n",
       "count  181.000000  181.000000  181.000000  \n",
       "mean     0.064898    0.064639    0.055676  \n",
       "std      1.257045    1.278832    1.279609  \n",
       "min     -2.970360   -2.957012   -3.071337  \n",
       "25%     -0.934296   -0.920635   -0.931001  \n",
       "50%      0.035938    0.029757    0.029757  \n",
       "75%      1.053081    1.072255    1.081842  \n",
       "max      3.574488    3.844715    3.574488  \n",
       "\n",
       "[8 rows x 901 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#statistical info of the data\n",
    "df_trainset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "\n",
      "           t1        t2        t3        t4        t5        t6        t7  \\\n",
      "0    1.660505  1.739092  1.812766  1.847148  1.901176  1.935558  1.906088   \n",
      "1   -0.379133  0.242145 -0.517195 -0.033979  0.587299 -0.517195 -0.172040   \n",
      "2    0.534425  0.444349  0.399312  0.511906  0.669539  0.714577  0.511906   \n",
      "3   -2.438882 -2.412564 -2.438882 -2.333611 -2.267818 -2.307294 -2.412564   \n",
      "4    1.601259  1.601259  1.589440  1.589440  1.589440  1.589440  1.589440   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "176 -0.816431 -0.804662 -0.706590 -0.612441 -0.624210 -0.624210 -0.628132   \n",
      "177 -3.739104 -3.719033 -3.731076 -3.715019 -3.755160 -3.751146 -3.803328   \n",
      "178 -1.010301 -1.151468 -1.201885 -1.232135 -1.332969 -1.353136 -1.403553   \n",
      "179  1.511671  1.577663  1.569414  1.618907  1.618907  1.602410  1.635405   \n",
      "180  0.732443  0.698494  0.694251  0.673033  0.711225  0.723955  0.728199   \n",
      "\n",
      "           t8        t9       t10  ...      t891      t892      t893  \\\n",
      "0    1.945381  1.925734  1.915911  ... -0.363097 -0.422037 -0.397478   \n",
      "1    0.035052  0.518269 -0.103009  ... -3.485523 -3.623585 -2.311998   \n",
      "2    0.692058  0.489387  0.556944  ...  3.799677  3.934791  3.754639   \n",
      "3   -2.162547 -2.241500 -2.254659  ... -1.715148 -1.767783 -1.794101   \n",
      "4    1.577622  1.577622  1.577622  ...  3.445002  3.208625  3.244082   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "176 -0.635978 -0.518292 -0.518292  ... -1.769692 -1.757923 -1.734386   \n",
      "177 -3.799314 -3.731076 -3.807342  ...  0.186612  0.150486  0.126402   \n",
      "178 -1.383386 -1.393470 -1.393470  ...  1.702134  1.702134  1.712217   \n",
      "179  1.511671  1.552916  1.561165  ...  0.455808  0.505302  0.455808   \n",
      "180  0.766391  0.736686  0.783365  ... -0.472727 -0.485458 -0.472727   \n",
      "\n",
      "         t894      t895      t896      t897      t898      t899      t900  \n",
      "0   -0.402390 -0.402390 -0.402390 -0.441683 -0.431860 -0.495712 -0.505535  \n",
      "1   -3.278430 -2.864245 -3.278430 -3.002307 -2.864245 -2.726183 -3.071337  \n",
      "2    4.024867  3.799677  3.777158  3.777158  3.574488  3.844715  3.574488  \n",
      "3   -1.688830 -1.701989 -1.701989 -1.767783 -1.767783 -1.754624 -1.767783  \n",
      "4    3.125893  3.031342  2.948610  2.854059  2.783146  2.794965  2.854059  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "176 -1.714772 -1.714772 -1.718695 -1.726540 -1.734386 -1.742232 -1.730463  \n",
      "177  0.182598  0.190626  0.174570  0.210696  0.210696  0.210696  0.250836  \n",
      "178  1.712217  1.712217  1.722301  1.722301  1.732384  1.732384  1.732384  \n",
      "179  0.381568  0.406315  0.315577  0.257834  0.266083  0.183594  0.142349  \n",
      "180 -0.489702 -0.549111 -0.583060 -0.608521 -0.608521 -0.600034 -0.617008  \n",
      "\n",
      "[181 rows x 900 columns]\n",
      "\n",
      "y_train:\n",
      "\n",
      "0      1.0\n",
      "1      1.0\n",
      "2      1.0\n",
      "3      1.0\n",
      "4      1.0\n",
      "      ... \n",
      "176    2.0\n",
      "177    2.0\n",
      "178    2.0\n",
      "179    2.0\n",
      "180    2.0\n",
      "Name: class, Length: 181, dtype: float64\n",
      "\n",
      "X_test:\n",
      "\n",
      "          t1        t2        t3        t4        t5        t6        t7  \\\n",
      "0  -0.778589 -0.744436 -0.725462 -0.702693 -0.714077 -0.721667 -0.733052   \n",
      "1   0.017272 -0.071910 -0.083542 -0.091297 -0.106807 -0.149459 -0.215376   \n",
      "2   0.083460  0.074729  0.079095  0.083460  0.135847  0.179503  0.236255   \n",
      "3  -1.298560 -1.171976 -1.087587 -0.991141 -0.997169 -1.015253 -1.009225   \n",
      "4  -0.576294 -0.594890 -0.617206 -0.654399 -0.672995 -0.680433 -0.717626   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "72  1.306131  1.293378  1.306131  1.312507  1.318884  1.293378  1.287001   \n",
      "73 -1.592920 -1.592920 -1.592920 -1.592920 -1.592920 -1.519859 -1.446798   \n",
      "74 -4.539753 -4.346270 -4.447618 -4.355484 -4.198854 -4.189641 -4.088292   \n",
      "75 -3.739104 -3.719033 -3.731076 -3.715019 -3.755160 -3.751146 -3.803328   \n",
      "76 -0.816431 -0.804662 -0.706590 -0.612441 -0.624210 -0.624210 -0.628132   \n",
      "\n",
      "          t8        t9       t10  ...      t891      t892      t893      t894  \\\n",
      "0  -0.717872 -0.736846 -0.763410  ...  0.082832  0.052474 -0.042396 -0.099319   \n",
      "1  -0.211499 -0.199866 -0.215376  ...  0.668686  0.684196  0.672564  0.722971   \n",
      "2   0.319201  0.419610  0.437072  ...  0.476362  0.454534  0.458900  0.471997   \n",
      "3  -0.979086 -0.991141 -0.954975  ... -0.563166 -0.502888 -0.502888 -0.412471   \n",
      "4  -0.747380 -0.751099 -0.792011  ... -1.379653 -1.372214 -1.372214 -1.320145   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "72  1.261495  1.287001  1.280625  ... -1.257230 -1.257230 -1.257230 -1.263607   \n",
      "73 -1.428532 -1.392001 -1.428532  ... -0.241285 -0.350877 -0.296081 -0.131693   \n",
      "74 -3.996157 -3.894809 -3.811888  ...  1.310814  1.273960  1.264746  1.237106   \n",
      "75 -3.799314 -3.731076 -3.807342  ...  0.186612  0.150486  0.126402  0.182598   \n",
      "76 -0.635978 -0.518292 -0.518292  ... -1.769692 -1.757923 -1.734386 -1.714772   \n",
      "\n",
      "        t895      t896      t897      t898      t899      t900  \n",
      "0  -0.125882 -0.190394 -0.228342 -0.205573 -0.273880 -0.323212  \n",
      "1   0.742358  0.753991  0.781133  0.788888  0.796643  0.804398  \n",
      "2   0.489459  0.476362  0.489459  0.493824  0.489459  0.476362  \n",
      "3  -0.370276 -0.297942 -0.231636 -0.183413 -0.123135 -0.099024  \n",
      "4  -1.312706 -1.286672 -1.268075 -1.238321 -1.193690 -1.145340  \n",
      "..       ...       ...       ...       ...       ...       ...  \n",
      "72 -1.263607 -1.263607 -1.263607 -1.263607 -1.263607 -1.263607  \n",
      "73 -0.204754 -0.204754 -0.259550 -0.314346 -0.369143 -0.423939  \n",
      "74  1.218679  1.200252  1.181825  1.089690  0.988341  0.997555  \n",
      "75  0.190626  0.174570  0.210696  0.210696  0.210696  0.250836  \n",
      "76 -1.714772 -1.718695 -1.726540 -1.734386 -1.742232 -1.730463  \n",
      "\n",
      "[77 rows x 900 columns]\n",
      "\n",
      "y_test:\n",
      "\n",
      "0     1.0\n",
      "1     1.0\n",
      "2     1.0\n",
      "3     1.0\n",
      "4     1.0\n",
      "     ... \n",
      "72    2.0\n",
      "73    2.0\n",
      "74    2.0\n",
      "75    2.0\n",
      "76    2.0\n",
      "Name: class, Length: 77, dtype: float64\n",
      "(181, 900)\n"
     ]
    }
   ],
   "source": [
    "#Here, we split the data into X and y\n",
    "#y is the target variable 'class', and X is everything else\n",
    "\n",
    "y_train = df_trainset['class']\n",
    "#X_test = X_test.transpose()\n",
    "X_train = df_trainset.drop(['class'], axis=1)\n",
    "y_test = df_testset['class']\n",
    "#y_test = y_test.transpose()\n",
    "X_test = df_testset.drop(['class'], axis=1)\n",
    "\n",
    "print(\"X_train:\\n\")\n",
    "print(X_train)\n",
    "print(\"\\ny_train:\\n\")\n",
    "print(y_train)\n",
    "print(\"\\nX_test:\\n\")\n",
    "print(X_test)\n",
    "print(\"\\ny_test:\\n\")\n",
    "print(y_test)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set duplicates: 23\n",
      "Test set duplicates: 0\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#We dont think duplicates are bad here, we just want to be aware of them\n",
    "\n",
    "#check for duplicates training set\n",
    "print(\"Train set duplicates:\",df_trainset.duplicated().sum())\n",
    "\n",
    "#check for duplicates test set\n",
    "print(\"Test set duplicates:\",df_testset.duplicated().sum())\n",
    "\n",
    "#Checking for nulls - there are none\n",
    "\n",
    "#check for null values in the entire train set dataframe\n",
    "print(df_trainset.isnull().any().any())\n",
    "\n",
    "#check for null values in the entire test set dataframe\n",
    "print(df_testset.isnull().any().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>...</th>\n",
       "      <th>t891</th>\n",
       "      <th>t892</th>\n",
       "      <th>t893</th>\n",
       "      <th>t894</th>\n",
       "      <th>t895</th>\n",
       "      <th>t896</th>\n",
       "      <th>t897</th>\n",
       "      <th>t898</th>\n",
       "      <th>t899</th>\n",
       "      <th>t900</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.054762</td>\n",
       "      <td>-0.055049</td>\n",
       "      <td>-0.053320</td>\n",
       "      <td>-0.044472</td>\n",
       "      <td>-0.046638</td>\n",
       "      <td>-0.054311</td>\n",
       "      <td>-0.060297</td>\n",
       "      <td>-0.064308</td>\n",
       "      <td>-0.059014</td>\n",
       "      <td>-0.060587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097132</td>\n",
       "      <td>0.101416</td>\n",
       "      <td>0.102469</td>\n",
       "      <td>0.088442</td>\n",
       "      <td>0.087919</td>\n",
       "      <td>0.075541</td>\n",
       "      <td>0.070701</td>\n",
       "      <td>0.064898</td>\n",
       "      <td>0.064639</td>\n",
       "      <td>0.055676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.231458</td>\n",
       "      <td>1.217009</td>\n",
       "      <td>1.212576</td>\n",
       "      <td>1.211890</td>\n",
       "      <td>1.212344</td>\n",
       "      <td>1.211266</td>\n",
       "      <td>1.216288</td>\n",
       "      <td>1.214133</td>\n",
       "      <td>1.209916</td>\n",
       "      <td>1.214343</td>\n",
       "      <td>...</td>\n",
       "      <td>1.280118</td>\n",
       "      <td>1.284467</td>\n",
       "      <td>1.264096</td>\n",
       "      <td>1.265234</td>\n",
       "      <td>1.264899</td>\n",
       "      <td>1.274256</td>\n",
       "      <td>1.260681</td>\n",
       "      <td>1.257045</td>\n",
       "      <td>1.278832</td>\n",
       "      <td>1.279609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.739104</td>\n",
       "      <td>-3.719033</td>\n",
       "      <td>-3.731076</td>\n",
       "      <td>-3.715019</td>\n",
       "      <td>-3.755160</td>\n",
       "      <td>-3.751146</td>\n",
       "      <td>-3.803328</td>\n",
       "      <td>-3.799314</td>\n",
       "      <td>-3.731076</td>\n",
       "      <td>-3.807342</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.485523</td>\n",
       "      <td>-3.623585</td>\n",
       "      <td>-3.027088</td>\n",
       "      <td>-3.278430</td>\n",
       "      <td>-3.003730</td>\n",
       "      <td>-3.278430</td>\n",
       "      <td>-3.002307</td>\n",
       "      <td>-2.970360</td>\n",
       "      <td>-2.957012</td>\n",
       "      <td>-3.071337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.854008</td>\n",
       "      <td>-0.831337</td>\n",
       "      <td>-0.804410</td>\n",
       "      <td>-0.804410</td>\n",
       "      <td>-0.855404</td>\n",
       "      <td>-0.822698</td>\n",
       "      <td>-0.850160</td>\n",
       "      <td>-0.883371</td>\n",
       "      <td>-0.870900</td>\n",
       "      <td>-0.896727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.904257</td>\n",
       "      <td>-0.882800</td>\n",
       "      <td>-0.877030</td>\n",
       "      <td>-0.874598</td>\n",
       "      <td>-0.849661</td>\n",
       "      <td>-0.846069</td>\n",
       "      <td>-0.849122</td>\n",
       "      <td>-0.934296</td>\n",
       "      <td>-0.920635</td>\n",
       "      <td>-0.931001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.034666</td>\n",
       "      <td>-0.055647</td>\n",
       "      <td>-0.033979</td>\n",
       "      <td>-0.012762</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>-0.038728</td>\n",
       "      <td>-0.014604</td>\n",
       "      <td>-0.003048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101768</td>\n",
       "      <td>0.072891</td>\n",
       "      <td>0.021509</td>\n",
       "      <td>0.045957</td>\n",
       "      <td>0.011206</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>-0.004682</td>\n",
       "      <td>0.035938</td>\n",
       "      <td>0.029757</td>\n",
       "      <td>0.029757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.764857</td>\n",
       "      <td>0.701450</td>\n",
       "      <td>0.795012</td>\n",
       "      <td>0.845768</td>\n",
       "      <td>0.863678</td>\n",
       "      <td>0.849089</td>\n",
       "      <td>0.857694</td>\n",
       "      <td>0.859221</td>\n",
       "      <td>0.853187</td>\n",
       "      <td>0.819685</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130934</td>\n",
       "      <td>1.173048</td>\n",
       "      <td>1.112793</td>\n",
       "      <td>1.082254</td>\n",
       "      <td>1.079085</td>\n",
       "      <td>1.094289</td>\n",
       "      <td>1.049968</td>\n",
       "      <td>1.053081</td>\n",
       "      <td>1.072255</td>\n",
       "      <td>1.081842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.482405</td>\n",
       "      <td>3.295940</td>\n",
       "      <td>3.109476</td>\n",
       "      <td>2.923011</td>\n",
       "      <td>2.736547</td>\n",
       "      <td>2.603414</td>\n",
       "      <td>2.583677</td>\n",
       "      <td>2.563941</td>\n",
       "      <td>2.542011</td>\n",
       "      <td>2.522274</td>\n",
       "      <td>...</td>\n",
       "      <td>3.799677</td>\n",
       "      <td>3.934791</td>\n",
       "      <td>3.754639</td>\n",
       "      <td>4.024867</td>\n",
       "      <td>3.799677</td>\n",
       "      <td>3.777158</td>\n",
       "      <td>3.777158</td>\n",
       "      <td>3.574488</td>\n",
       "      <td>3.844715</td>\n",
       "      <td>3.574488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 900 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               t1          t2          t3          t4          t5          t6  \\\n",
       "count  181.000000  181.000000  181.000000  181.000000  181.000000  181.000000   \n",
       "mean    -0.054762   -0.055049   -0.053320   -0.044472   -0.046638   -0.054311   \n",
       "std      1.231458    1.217009    1.212576    1.211890    1.212344    1.211266   \n",
       "min     -3.739104   -3.719033   -3.731076   -3.715019   -3.755160   -3.751146   \n",
       "25%     -0.854008   -0.831337   -0.804410   -0.804410   -0.855404   -0.822698   \n",
       "50%      0.004370   -0.034666   -0.055647   -0.033979   -0.012762    0.003513   \n",
       "75%      0.764857    0.701450    0.795012    0.845768    0.863678    0.849089   \n",
       "max      3.482405    3.295940    3.109476    2.923011    2.736547    2.603414   \n",
       "\n",
       "               t7          t8          t9         t10  ...        t891  \\\n",
       "count  181.000000  181.000000  181.000000  181.000000  ...  181.000000   \n",
       "mean    -0.060297   -0.064308   -0.059014   -0.060587  ...    0.097132   \n",
       "std      1.216288    1.214133    1.209916    1.214343  ...    1.280118   \n",
       "min     -3.803328   -3.799314   -3.731076   -3.807342  ...   -3.485523   \n",
       "25%     -0.850160   -0.883371   -0.870900   -0.896727  ...   -0.904257   \n",
       "50%      0.003733   -0.038728   -0.014604   -0.003048  ...    0.101768   \n",
       "75%      0.857694    0.859221    0.853187    0.819685  ...    1.130934   \n",
       "max      2.583677    2.563941    2.542011    2.522274  ...    3.799677   \n",
       "\n",
       "             t892        t893        t894        t895        t896        t897  \\\n",
       "count  181.000000  181.000000  181.000000  181.000000  181.000000  181.000000   \n",
       "mean     0.101416    0.102469    0.088442    0.087919    0.075541    0.070701   \n",
       "std      1.284467    1.264096    1.265234    1.264899    1.274256    1.260681   \n",
       "min     -3.623585   -3.027088   -3.278430   -3.003730   -3.278430   -3.002307   \n",
       "25%     -0.882800   -0.877030   -0.874598   -0.849661   -0.846069   -0.849122   \n",
       "50%      0.072891    0.021509    0.045957    0.011206    0.016229   -0.004682   \n",
       "75%      1.173048    1.112793    1.082254    1.079085    1.094289    1.049968   \n",
       "max      3.934791    3.754639    4.024867    3.799677    3.777158    3.777158   \n",
       "\n",
       "             t898        t899        t900  \n",
       "count  181.000000  181.000000  181.000000  \n",
       "mean     0.064898    0.064639    0.055676  \n",
       "std      1.257045    1.278832    1.279609  \n",
       "min     -2.970360   -2.957012   -3.071337  \n",
       "25%     -0.934296   -0.920635   -0.931001  \n",
       "50%      0.035938    0.029757    0.029757  \n",
       "75%      1.053081    1.072255    1.081842  \n",
       "max      3.574488    3.844715    3.574488  \n",
       "\n",
       "[8 rows x 900 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#statistical info of the train data\n",
    "X_train.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>t3</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>t6</th>\n",
       "      <th>t7</th>\n",
       "      <th>t8</th>\n",
       "      <th>t9</th>\n",
       "      <th>t10</th>\n",
       "      <th>...</th>\n",
       "      <th>t891</th>\n",
       "      <th>t892</th>\n",
       "      <th>t893</th>\n",
       "      <th>t894</th>\n",
       "      <th>t895</th>\n",
       "      <th>t896</th>\n",
       "      <th>t897</th>\n",
       "      <th>t898</th>\n",
       "      <th>t899</th>\n",
       "      <th>t900</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.199686</td>\n",
       "      <td>-0.196386</td>\n",
       "      <td>-0.188138</td>\n",
       "      <td>-0.185611</td>\n",
       "      <td>-0.183856</td>\n",
       "      <td>-0.178102</td>\n",
       "      <td>-0.179701</td>\n",
       "      <td>-0.187306</td>\n",
       "      <td>-0.187474</td>\n",
       "      <td>-0.183142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155905</td>\n",
       "      <td>-0.149780</td>\n",
       "      <td>-0.158325</td>\n",
       "      <td>-0.144326</td>\n",
       "      <td>-0.137616</td>\n",
       "      <td>-0.133673</td>\n",
       "      <td>-0.124801</td>\n",
       "      <td>-0.123033</td>\n",
       "      <td>-0.117639</td>\n",
       "      <td>-0.110963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.476630</td>\n",
       "      <td>1.432874</td>\n",
       "      <td>1.421172</td>\n",
       "      <td>1.387524</td>\n",
       "      <td>1.368842</td>\n",
       "      <td>1.353518</td>\n",
       "      <td>1.341355</td>\n",
       "      <td>1.334124</td>\n",
       "      <td>1.309845</td>\n",
       "      <td>1.304831</td>\n",
       "      <td>...</td>\n",
       "      <td>1.083054</td>\n",
       "      <td>1.067969</td>\n",
       "      <td>1.067737</td>\n",
       "      <td>1.062268</td>\n",
       "      <td>1.060778</td>\n",
       "      <td>1.066403</td>\n",
       "      <td>1.054360</td>\n",
       "      <td>1.059032</td>\n",
       "      <td>1.056025</td>\n",
       "      <td>1.051109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.539753</td>\n",
       "      <td>-4.346270</td>\n",
       "      <td>-4.447618</td>\n",
       "      <td>-4.355484</td>\n",
       "      <td>-4.198854</td>\n",
       "      <td>-4.189641</td>\n",
       "      <td>-4.088292</td>\n",
       "      <td>-3.996157</td>\n",
       "      <td>-3.894809</td>\n",
       "      <td>-3.811888</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.860785</td>\n",
       "      <td>-2.599652</td>\n",
       "      <td>-2.559477</td>\n",
       "      <td>-2.418867</td>\n",
       "      <td>-2.401751</td>\n",
       "      <td>-2.437829</td>\n",
       "      <td>-2.469898</td>\n",
       "      <td>-2.526019</td>\n",
       "      <td>-2.550071</td>\n",
       "      <td>-2.558089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.164998</td>\n",
       "      <td>-1.161052</td>\n",
       "      <td>-1.129835</td>\n",
       "      <td>-1.041658</td>\n",
       "      <td>-1.043875</td>\n",
       "      <td>-1.023699</td>\n",
       "      <td>-1.009225</td>\n",
       "      <td>-1.010499</td>\n",
       "      <td>-1.065569</td>\n",
       "      <td>-1.005302</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.049814</td>\n",
       "      <td>-1.010397</td>\n",
       "      <td>-0.942657</td>\n",
       "      <td>-0.932570</td>\n",
       "      <td>-0.902307</td>\n",
       "      <td>-0.820113</td>\n",
       "      <td>-0.789454</td>\n",
       "      <td>-0.803023</td>\n",
       "      <td>-0.869985</td>\n",
       "      <td>-0.922850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.232664</td>\n",
       "      <td>-0.221682</td>\n",
       "      <td>-0.179368</td>\n",
       "      <td>-0.185413</td>\n",
       "      <td>-0.166084</td>\n",
       "      <td>-0.149459</td>\n",
       "      <td>-0.185413</td>\n",
       "      <td>-0.199637</td>\n",
       "      <td>-0.184009</td>\n",
       "      <td>-0.148845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146548</td>\n",
       "      <td>-0.121943</td>\n",
       "      <td>-0.168377</td>\n",
       "      <td>-0.117868</td>\n",
       "      <td>-0.125882</td>\n",
       "      <td>-0.164813</td>\n",
       "      <td>-0.228342</td>\n",
       "      <td>-0.183413</td>\n",
       "      <td>-0.170054</td>\n",
       "      <td>-0.099024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.758231</td>\n",
       "      <td>0.767264</td>\n",
       "      <td>0.747506</td>\n",
       "      <td>0.634862</td>\n",
       "      <td>0.513345</td>\n",
       "      <td>0.544512</td>\n",
       "      <td>0.638011</td>\n",
       "      <td>0.655820</td>\n",
       "      <td>0.542029</td>\n",
       "      <td>0.504104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476362</td>\n",
       "      <td>0.584308</td>\n",
       "      <td>0.573599</td>\n",
       "      <td>0.580146</td>\n",
       "      <td>0.549823</td>\n",
       "      <td>0.562566</td>\n",
       "      <td>0.555120</td>\n",
       "      <td>0.614483</td>\n",
       "      <td>0.622132</td>\n",
       "      <td>0.679130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.735155</td>\n",
       "      <td>3.604026</td>\n",
       "      <td>3.717671</td>\n",
       "      <td>3.437931</td>\n",
       "      <td>3.542833</td>\n",
       "      <td>3.534091</td>\n",
       "      <td>3.402963</td>\n",
       "      <td>3.455414</td>\n",
       "      <td>3.210642</td>\n",
       "      <td>3.184416</td>\n",
       "      <td>...</td>\n",
       "      <td>2.303505</td>\n",
       "      <td>2.281459</td>\n",
       "      <td>2.272302</td>\n",
       "      <td>2.366447</td>\n",
       "      <td>2.466869</td>\n",
       "      <td>2.592396</td>\n",
       "      <td>2.504527</td>\n",
       "      <td>2.510804</td>\n",
       "      <td>2.535909</td>\n",
       "      <td>2.523356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 900 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              t1         t2         t3         t4         t5         t6  \\\n",
       "count  77.000000  77.000000  77.000000  77.000000  77.000000  77.000000   \n",
       "mean   -0.199686  -0.196386  -0.188138  -0.185611  -0.183856  -0.178102   \n",
       "std     1.476630   1.432874   1.421172   1.387524   1.368842   1.353518   \n",
       "min    -4.539753  -4.346270  -4.447618  -4.355484  -4.198854  -4.189641   \n",
       "25%    -1.164998  -1.161052  -1.129835  -1.041658  -1.043875  -1.023699   \n",
       "50%    -0.232664  -0.221682  -0.179368  -0.185413  -0.166084  -0.149459   \n",
       "75%     0.758231   0.767264   0.747506   0.634862   0.513345   0.544512   \n",
       "max     3.735155   3.604026   3.717671   3.437931   3.542833   3.534091   \n",
       "\n",
       "              t7         t8         t9        t10  ...       t891       t892  \\\n",
       "count  77.000000  77.000000  77.000000  77.000000  ...  77.000000  77.000000   \n",
       "mean   -0.179701  -0.187306  -0.187474  -0.183142  ...  -0.155905  -0.149780   \n",
       "std     1.341355   1.334124   1.309845   1.304831  ...   1.083054   1.067969   \n",
       "min    -4.088292  -3.996157  -3.894809  -3.811888  ...  -2.860785  -2.599652   \n",
       "25%    -1.009225  -1.010499  -1.065569  -1.005302  ...  -1.049814  -1.010397   \n",
       "50%    -0.185413  -0.199637  -0.184009  -0.148845  ...  -0.146548  -0.121943   \n",
       "75%     0.638011   0.655820   0.542029   0.504104  ...   0.476362   0.584308   \n",
       "max     3.402963   3.455414   3.210642   3.184416  ...   2.303505   2.281459   \n",
       "\n",
       "            t893       t894       t895       t896       t897       t898  \\\n",
       "count  77.000000  77.000000  77.000000  77.000000  77.000000  77.000000   \n",
       "mean   -0.158325  -0.144326  -0.137616  -0.133673  -0.124801  -0.123033   \n",
       "std     1.067737   1.062268   1.060778   1.066403   1.054360   1.059032   \n",
       "min    -2.559477  -2.418867  -2.401751  -2.437829  -2.469898  -2.526019   \n",
       "25%    -0.942657  -0.932570  -0.902307  -0.820113  -0.789454  -0.803023   \n",
       "50%    -0.168377  -0.117868  -0.125882  -0.164813  -0.228342  -0.183413   \n",
       "75%     0.573599   0.580146   0.549823   0.562566   0.555120   0.614483   \n",
       "max     2.272302   2.366447   2.466869   2.592396   2.504527   2.510804   \n",
       "\n",
       "            t899       t900  \n",
       "count  77.000000  77.000000  \n",
       "mean   -0.117639  -0.110963  \n",
       "std     1.056025   1.051109  \n",
       "min    -2.550071  -2.558089  \n",
       "25%    -0.869985  -0.922850  \n",
       "50%    -0.170054  -0.099024  \n",
       "75%     0.622132   0.679130  \n",
       "max     2.535909   2.523356  \n",
       "\n",
       "[8 rows x 900 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#statistical info of the test data\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Outliers\n",
    "Identify outliers and anomalies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers \n",
      "      class  t1        t2        t3        t4       t5        t6        t7  \\\n",
      "0      NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "1      NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "2      NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "3      NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "4      NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "..     ...  ..       ...       ...       ...      ...       ...       ...   \n",
      "176    NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "177    NaN NaN -3.719033 -3.731076 -3.715019 -3.75516 -3.751146 -3.803328   \n",
      "178    NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "179    NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "180    NaN NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
      "\n",
      "           t8        t9  ...  t891  t892  t893      t894  t895  t896  t897  \\\n",
      "0         NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "1         NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "2         NaN       NaN  ...   NaN   NaN   NaN  4.024867   NaN   NaN   NaN   \n",
      "3         NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "4         NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "..        ...       ...  ...   ...   ...   ...       ...   ...   ...   ...   \n",
      "176       NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "177 -3.799314 -3.731076  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "178       NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "179       NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "180       NaN       NaN  ...   NaN   NaN   NaN       NaN   NaN   NaN   NaN   \n",
      "\n",
      "     t898  t899  t900  \n",
      "0     NaN   NaN   NaN  \n",
      "1     NaN   NaN   NaN  \n",
      "2     NaN   NaN   NaN  \n",
      "3     NaN   NaN   NaN  \n",
      "4     NaN   NaN   NaN  \n",
      "..    ...   ...   ...  \n",
      "176   NaN   NaN   NaN  \n",
      "177   NaN   NaN   NaN  \n",
      "178   NaN   NaN   NaN  \n",
      "179   NaN   NaN   NaN  \n",
      "180   NaN   NaN   NaN  \n",
      "\n",
      "[181 rows x 901 columns] \n",
      "\n",
      "outliers count \n",
      " 238 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFor now, vou esquever isto\\nVou normalizar o data set, e depois, se tivermos más classificações, volot a isto\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the z-score for each point of the training set\n",
    "z_scores = np.abs((df_trainset - df_trainset.mean()) / df_trainset.std())\n",
    "\n",
    "#define a threshold value\n",
    "threshold = 3 # its considered an outiler when the value of the point is 3 * mean of the training set, so the threshold is 3\n",
    "\n",
    "#Identify the outliers\n",
    "outliers = df_trainset[z_scores > threshold]\n",
    "\n",
    "#Count the number of outliers\n",
    "num_outliers = outliers.count().sum()\n",
    "\n",
    "\n",
    "print(f\"outliers \\n {outliers} \\n\") # non null values represent the outliers\n",
    "print(f\"outliers count \\n {num_outliers} \\n\")\n",
    "'''\n",
    "For now, vou esquever isto\n",
    "Vou normalizar o data set, e depois, se tivermos más classificações, volot a isto\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlations Matrix\n",
      "\n",
      "class    1.000000\n",
      "t56      0.183297\n",
      "t54      0.181402\n",
      "t55      0.177794\n",
      "t57      0.175693\n",
      "           ...   \n",
      "t219    -0.206140\n",
      "t216    -0.209983\n",
      "t215    -0.211973\n",
      "t218    -0.216237\n",
      "t217    -0.217713\n",
      "Name: class, Length: 901, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#creates a matrix of correlations\n",
    "corr_matrix = df_trainset.corr() \n",
    "#how much each attribute correlates with the Class target variable value, the lower the value the least relevant the feature is\n",
    "print(\"\\nCorrelations Matrix\\n\")\n",
    "print(corr_matrix['class'].sort_values(ascending=False))#to present all columns their type cannot be object so we must convert it to float\n",
    "\n",
    "#plot to present the correlation between the 2 most correlated features with target variable\n",
    "#sns.pairplot(df, vars=['Y9', 'Y10'], hue='Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.66142837]\n",
      "  [ 1.74005857]\n",
      "  [ 1.81377433]\n",
      "  ...\n",
      "  [-0.43210013]\n",
      "  [-0.49598715]\n",
      "  [-0.50581593]]\n",
      "\n",
      " [[-0.37934378]\n",
      "  [ 0.24227965]\n",
      "  [-0.51748231]\n",
      "  ...\n",
      "  [-2.86583748]\n",
      "  [-2.72769891]\n",
      "  [-3.07304522]]\n",
      "\n",
      " [[ 0.53472256]\n",
      "  [ 0.44459656]\n",
      "  [ 0.39953356]\n",
      "  ...\n",
      "  [ 3.57647509]\n",
      "  [ 3.84685304]\n",
      "  [ 3.57647509]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.01086234]\n",
      "  [-1.15210843]\n",
      "  [-1.20255346]\n",
      "  ...\n",
      "  [ 1.73334731]\n",
      "  [ 1.73334731]\n",
      "  [ 1.73334731]]\n",
      "\n",
      " [[ 1.51251182]\n",
      "  [ 1.57853991]\n",
      "  [ 1.57028643]\n",
      "  ...\n",
      "  [ 0.26623091]\n",
      "  [ 0.18369575]\n",
      "  [ 0.14242817]]\n",
      "\n",
      " [[ 0.73284978]\n",
      "  [ 0.69888245]\n",
      "  [ 0.69463653]\n",
      "  ...\n",
      "  [-0.6088596 ]\n",
      "  [-0.60036777]\n",
      "  [-0.61735143]]]\n",
      "\n",
      "\n",
      "[[[-0.77902211]\n",
      "  [-0.74484985]\n",
      "  [-0.72586526]\n",
      "  ...\n",
      "  [-0.20568749]\n",
      "  [-0.27403201]\n",
      "  [-0.32339195]]\n",
      "\n",
      " [[ 0.01728157]\n",
      "  [-0.07194975]\n",
      "  [-0.08358862]\n",
      "  ...\n",
      "  [ 0.78932646]\n",
      "  [ 0.7970857 ]\n",
      "  [ 0.80484494]]\n",
      "\n",
      " [[ 0.08350654]\n",
      "  [ 0.07477053]\n",
      "  [ 0.07913854]\n",
      "  ...\n",
      "  [ 0.49409897]\n",
      "  [ 0.48973097]\n",
      "  [ 0.47662696]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-4.54227759]\n",
      "  [-4.34868671]\n",
      "  [-4.45009146]\n",
      "  ...\n",
      "  [ 1.09029579]\n",
      "  [ 0.98889101]\n",
      "  [ 0.99810963]]\n",
      "\n",
      " [[-3.74118251]\n",
      "  [-3.72110125]\n",
      "  [-3.73315005]\n",
      "  ...\n",
      "  [ 0.21081327]\n",
      "  [ 0.21081327]\n",
      "  [ 0.25097583]]\n",
      "\n",
      " [[-0.81688488]\n",
      "  [-0.80510968]\n",
      "  [-0.70698301]\n",
      "  ...\n",
      "  [-1.73535045]\n",
      "  [-1.74320061]\n",
      "  [-1.73142537]]]\n"
     ]
    }
   ],
   "source": [
    "#NORMALIZE THE DATA TO COMPARE TIME SERIES \n",
    "\n",
    "X_train_normalized = TimeSeriesScalerMeanVariance(mu=0, std=1).fit_transform(X_train)\n",
    "\n",
    "X_test_normalized = TimeSeriesScalerMeanVariance(mu=0, std=1).fit_transform(X_test)\n",
    "\n",
    "\n",
    "print(X_train_normalized)\n",
    "print(\"\\n\")\n",
    "print(X_test_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model/Representation Method for Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNeighborsTimeSeriesClassifier model implements the k-nearest neighbor for time series. \n",
    "\n",
    "We have three possible metrics, as seen below in comments\n",
    "* 1-NN with Euclidean distance\n",
    "* 1-NN with DTW\n",
    "* 1-NN with SAX, in this case you need to set two other parameters: `n_segments` and `alphabet_size_avg`. The first parameter means the number of Piecewise Aggregate Approximation pieces to compute (start by fixing it at 16) and the latter is the number of SAX symbols to use (start by fixing it at 10). To fix these parameters, you need to use the parameter `metric_params` in the class of the classifier and provide a dictionary with the two parameters required.\n",
    "\n",
    "We are going to use the accuracy score (from scikit-learn) to compare the methods. Also, our data is already splitted in train and test set, so we don't need to worry about splitting our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Precision is:  0.5455\n",
      "The Recall is:  0.7273\n",
      "The F1 score is:  0.6234\n",
      "The Matthews correlation coefficient is:  0.2727\n",
      "\n",
      "This is the Confusion Matrix\n",
      "    0   1\n",
      "0  24   9\n",
      "1  20  24\n"
     ]
    }
   ],
   "source": [
    "#K nearest Neighbors for time series\n",
    "\n",
    "#c = KNeighborsTimeSeriesClassifier(n_neighbors = 3, metric = 'euclidean')#0.6103896103896104\n",
    "c = KNeighborsTimeSeriesClassifier(n_neighbors = 1, metric = 'dtw') #0.6233766233766234\n",
    "#dict = {'n_segments' : 16 , 'alphabet_size_avg': 10}\n",
    "#c = KNeighborsTimeSeriesClassifier(n_neighbors = 1, metric = 'sax', metric_params = dict)#0.5974025974025974\n",
    "#c = SVC(C=50,gamma='auto')\n",
    "#c  = RandomForestClassifier(n_estimators=12, random_state=0)\n",
    "#c = LogisticRegression(C = 0.01)\n",
    "#c = DecisionTreeClassifier(criterion = 'gini')\n",
    "#c = KNeighborsClassifier(n_neighbors = 5, algorithm = 'ball_tree',weights = 'distance')\n",
    "#c = GaussianNB()\n",
    "\n",
    "c.fit(X_train, y_train)\n",
    "preds = c.predict(X_test)\n",
    "\n",
    "#accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "#print(accuracy)\n",
    "\n",
    "precision = precision_score(y_test, preds)\n",
    "recall = recall_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds)\n",
    "mcc = matthews_corrcoef(y_test, preds)\n",
    "\n",
    "\n",
    "print(\"The Precision is: %7.4f\" % precision)\n",
    "print(\"The Recall is: %7.4f\" % recall)\n",
    "print(\"The F1 score is: %7.4f\" % f1)\n",
    "print(\"The Matthews correlation coefficient is: %7.4f\" % mcc)\n",
    "print()\n",
    "print(\"This is the Confusion Matrix\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, preds)))\n",
    "\n",
    "#Test randomForest , SVM, decision trees , KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to explore some representation methods, namely the Piecewise Aggregate Approximation (PAA) and the Symbolic Aggregate Approximation (SAX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181, 10, 1)\n",
      "(77, 10, 1)\n",
      "0.6233766233766234\n"
     ]
    }
   ],
   "source": [
    "#piecewise aggregate Approximation\n",
    "\n",
    "paa = PiecewiseAggregateApproximation(n_segments=12)\n",
    "\n",
    "paa_X_train_data = paa.fit_transform(X_train_normalized)\n",
    "print(paa_X_train_data.shape)\n",
    "paa_X_test_data = paa.fit_transform(X_test_normalized)\n",
    "print(paa_X_test_data.shape)\n",
    "\n",
    "\n",
    "sk_c = KNeighborsClassifier(n_neighbors = 1)\n",
    "sk_c.fit(paa_X_train_data[:,:,0] ,y_train)\n",
    "\n",
    "preds = sk_c.predict(paa_X_test_data[:,:,0])\n",
    "\n",
    "accuracy_paa = accuracy_score(y_test, preds)\n",
    "\n",
    "print(accuracy_paa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181, 32, 1)\n",
      "(77, 32, 1)\n",
      "(77, 32)\n",
      "0.6753246753246753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 181 1-dimensional timeseries\n",
      "  warnings.warn(\n",
      "c:\\Users\\guilh\\anaconda3\\Lib\\site-packages\\tslearn\\utils\\utils.py:90: UserWarning: 2-Dimensional data passed. Assuming these are 77 1-dimensional timeseries\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#symbolic aggregate Approximation\n",
    "\n",
    "sax = SymbolicAggregateApproximation(n_segments=32, alphabet_size_avg=50)\n",
    "sax_X_train_data = sax.fit_transform(X_train)\n",
    "print(sax_X_train_data.shape)\n",
    "sax_X_test_data = sax.fit_transform(X_test)\n",
    "print(sax_X_test_data.shape)\n",
    "\n",
    "\n",
    "sk_c = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "sk_c.fit(sax_X_train_data[:,:,0] ,y_train)\n",
    "\n",
    "preds = sk_c.predict(sax_X_test_data[:,:,0])\n",
    "\n",
    "print(sax_X_test_data[:,:,0].shape)\n",
    "\n",
    "accuracy_sax = accuracy_score(y_test, preds)\n",
    "\n",
    "print(accuracy_sax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute list of scalers, imputers,models and present the results \n",
    "def test_models (scalers, representation_models, models, X_train, y_train, X_test,y_test, verbose, show_model):\n",
    "    results =[]\n",
    "    ct = 0\n",
    "    for name_scaler, scaler in scalers:\n",
    "        for name_rep_method, rep_method in representation_models:\n",
    "            for name_mod, model in models:\n",
    "                #scaling\n",
    "                scaler.fit(X_train)\n",
    "                Xt_train = scaler.transform(X_train)\n",
    "                Xt_test  = scaler.transform(X_test)\n",
    "\n",
    "                #representation methods\n",
    "                if verbose:\n",
    "                    rep_method.fit(Xt_train)\n",
    "                    Xt_train = rep_method.transform(Xt_train)\n",
    "                    Xt_test  = rep_method.transform(Xt_test)\n",
    "\n",
    "\n",
    "                model.fit(Xt_train[:,:,0], y_train) #[:,:,0] to have only 2 dimensions\n",
    "                preds = model.predict(Xt_test[:,:,0]) #PREDICTION\n",
    "\n",
    "                #present model number\n",
    "                if show_model:\n",
    "                    ct += 1\n",
    "                    print(\"\\nModel %d\" % ct)\n",
    "\n",
    "                #save results\n",
    "                results = save_results (name_scaler, scaler, name_rep_method, rep_method, name_mod, model, results,y_test, preds, show_model)\n",
    "    \n",
    "    results_sorted = sorted(results, key=lambda x: x[8], reverse=True) #f1 sorted decreasing\n",
    "    display_results(results_sorted, verbose)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Save the model scores and present intermediate results (w/ verbose)\n",
    "# Returns the list with the saved results \n",
    "def save_results(name_scaler, scaler,name_rep_method, rep_method, name_mod, model, results, y_test, preds, show_model):\n",
    "\n",
    "    # Calculate the precision, recall, f1 and mcc scores\n",
    "    precision = precision_score(y_test, preds)\n",
    "    recall = recall_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    mcc = matthews_corrcoef(y_test, preds)\n",
    "    \n",
    "    if show_model:\n",
    "        print(f\"Scaler: {scaler} imputer: {rep_method} classifier: {name_mod} {model}\")\n",
    "        print(\"The Precision is: %7.4f\" % precision)\n",
    "        print(\"The Recall is: %7.4f\" % recall)\n",
    "        print(\"The F1 score is: %7.4f\" % f1)\n",
    "        print(\"The Matthews correlation coefficient is: %7.4f\" % mcc)\n",
    "        print()\n",
    "        print(\"This is the Confusion Matrix\")\n",
    "        print(pd.DataFrame(confusion_matrix(y_test, preds)))\n",
    "\n",
    "\n",
    "    results.append((name_scaler,\n",
    "                    scaler,\n",
    "                    name_rep_method, \n",
    "                    rep_method, \n",
    "                    name_mod, \n",
    "                    model,\n",
    "                    precision,\n",
    "                    recall,\n",
    "                    f1,\n",
    "                    mcc,                    \n",
    "                    ))\n",
    "    return results\n",
    "\n",
    "# Display the model final results. Receives the ordered results to present\n",
    "def display_results (results, verbose):        \n",
    "    \n",
    "    noshow = \"\"\n",
    "    print (f\"\\n--------------------------Results for Classification Models Performance--------------------------\")\n",
    "    for res in results:\n",
    "        name_scaler = res [0]\n",
    "        scaler = res [1]\n",
    "        name_rep_method = res [2]\n",
    "        rep_method = res [3]\n",
    "        name_mod = res [4]\n",
    "        model = res [5]\n",
    "        precision = res [6]\n",
    "        recall = res [7]\n",
    "        f1 = res [8]\n",
    "        mcc = res [9]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{name_mod.ljust(25)} | precision     {precision:.4f} | recall     {recall:.4f} | f1     {f1:.4f}| mcc     {mcc:.4f}\")\n",
    "            print(f\"{noshow.ljust(25)} | scaler {scaler} | imputer {rep_method}\")\n",
    "        else:\n",
    "             print(f\"{name_mod.ljust(25)} | precision     {precision:.4f} | recall     {recall:.4f} | f1     {f1:.4f}| mcc     {mcc:.4f}\")\n",
    "             print(f\"{noshow.ljust(25)} | scaler {scaler}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Defining a list of scalers\n",
    "scalers = [\n",
    "    ('PowerTransformer', PowerTransformer()),\n",
    "    ('MinMaxScaler', MinMaxScaler()),\n",
    "    ('StandardScaler', StandardScaler())\n",
    "    ('TimeSeriesScalerMeanVariance', TimeSeriesScalerMeanVariance(mu=0, std=1))\n",
    "]\n",
    "\n",
    "representation_models = [\n",
    "    ('PiecewiseAggregateApproximation_ns10', PiecewiseAggregateApproximation(n_segments=12)),\n",
    "    ('PiecewiseAggregateApproximation_ns16', PiecewiseAggregateApproximation(n_segments=16)),\n",
    "    ('SymbolicAggregateApproximation_ns10', SymbolicAggregateApproximation(n_segments=10, alphabet_size_avg=40)),\n",
    "    ('SymbolicAggregateApproximation_ns10', SymbolicAggregateApproximation(n_segments=32, alphabet_size_avg=40))\n",
    "]\n",
    "\n",
    "dict = {'n_segments' : 16 , 'alphabet_size_avg': 10}\n",
    "\n",
    "#TO USE WITH THE REPRESENTATION METHODS\n",
    "# Defining a list of classification models, ALTER THE MODELS\n",
    "classification_models = [\n",
    "    ('LogisticRegression', LogisticRegression(C = 0.01)),\n",
    "    ('DecisionTree_maxd10', DecisionTreeClassifier(max_depth = 10)),\n",
    "    ('DecisionTree_minsl20', DecisionTreeClassifier(min_samples_leaf = 5)),\n",
    "    ('DecisionTree_critgini', DecisionTreeClassifier(criterion = 'gini')),\n",
    "    ('DecisionTree_critentropy', DecisionTreeClassifier(criterion = 'entropy')),\n",
    "    ('GaussianNB', GaussianNB()),\n",
    "    ('KNN_K1_wdist', KNeighborsClassifier(n_neighbors = 1,weights = 'distance')),\n",
    "    ('KNN_K1_eu', KNeighborsClassifier(n_neighbors = 1, metric = 'euclidean')),\n",
    "    ('KNN_K1_dtw', KNeighborsClassifier(n_neighbors = 1, metric = 'dtw')),\n",
    "    ('KNN_K1_sax', KNeighborsClassifier(n_neighbors = 1, metric = 'sax',metric_params = dict)),\n",
    "    ('RandomForestClassifier_ne50',RandomForestClassifier(n_estimators=50, random_state=0)),\n",
    "    ('RandomForestClassifier_ne10',RandomForestClassifier(n_estimators=10, random_state=0)),\n",
    "    ('SVC_c50',SVC(C=50,gamma='auto')),\n",
    "    ('SVC_c10',SVC(C=10,gamma='auto'))\n",
    "]\n",
    "\n",
    "VERBOSE = False #false will not present the representation methods\n",
    "SHOW_MODEL = True #True to present/print the progress of the model performance\n",
    "\n",
    "#First run will be just with the scalers and the classification models\n",
    "test_models (scalers, representation_models, classification_models, X_train, y_train, X_test,y_test, VERBOSE, SHOW_MODEL)\n",
    "\n",
    "VERBOSE = True\n",
    "\n",
    "#Second run, for representation models\n",
    "test_models (scalers, representation_models, classification_models, X_train, y_train, X_test,y_test, VERBOSE, SHOW_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2 of the project is most likely to be done using the info in the TP7 timeseries forecasting i believe \n",
    "\n",
    "#histograms in EDA its  NO GO, too many rows in the datasets\n",
    "#Same for presenting the plot for outliers, FIND ANOTHER WAY, counting them its a solution\n",
    "\n",
    "#DONT KNOW if i have to check the outliers of the test set also \n",
    "#VERIFY if a matrix of correlations is possibleor not, sounds difficult as we dont have a target variable\n",
    "\n",
    "#Scaling data might need to be done VERIFY  the 4 Scalers PowerTransformer, StandardScaler , MinMaxScaler and the last one is the normalizer i believe it might be useful as it works with rows check AA notes/slides, no need for imputation though\n",
    "#in the Project statement when she says find the best classifier model, in the TP6 she only uses the KNeighborsClassifier CHECK if its possible to use other classification models or if its even needed to\n",
    "\n",
    "#SEE IF SCALING IS NEEDED\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
